{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"hetzner-k3s The easiest and fastest way to createproduction-ready Kubernetes clusters on Hetzner Cloud <p>\u2764\ufe0f Support This Project</p> <p>hetzner-k3s is maintained by a single developer. If it saves you time or money, please consider sponsoring its continued development.</p> <p>Become a sponsor \u2192</p>"},{"location":"#what-is-hetzner-k3s","title":"What is hetzner-k3s?","text":"<p>hetzner-k3s is a CLI tool that creates fully-configured Kubernetes clusters on Hetzner Cloud in minutes. It uses k3s, a lightweight Kubernetes distribution by Rancher, and automatically configures everything you need for production workloads.</p>"},{"location":"#key-highlights","title":"Key Highlights","text":"Metric Value Time to create a 6-node HA cluster 2-3 minutes Tested scale 500 nodes in under 11 minutes Dependencies Just the CLI tool Platform fees None \u2014 you only pay Hetzner"},{"location":"#what-gets-installed-automatically","title":"What Gets Installed Automatically","text":"<ul> <li>k3s \u2014 lightweight, certified Kubernetes</li> <li>Hetzner Cloud Controller Manager \u2014 automatic load balancer provisioning</li> <li>Hetzner CSI Driver \u2014 persistent volumes via Hetzner block storage</li> <li>System Upgrade Controller \u2014 zero-downtime k3s upgrades</li> <li>Cluster Autoscaler \u2014 automatic node scaling based on demand</li> <li>Private networking and firewall \u2014 secure cluster communication</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"Installation Install hetzner-k3s on macOS, Linux, or Windows (via WSL) Create Your First Cluster Configuration reference and cluster creation Complete Tutorial Set up a cluster with ingress, TLS, and a sample application Why hetzner-k3s? Compare to managed services and Terraform-based alternatives"},{"location":"#why-choose-hetzner-k3s","title":"Why Choose hetzner-k3s?","text":""},{"location":"#speed-without-shortcuts","title":"Speed Without Shortcuts","text":"<p>A 3-master, 3-worker highly available cluster takes just 2-3 minutes to create. This includes provisioning all infrastructure (instances, load balancer, private network, firewall) and deploying k3s with all essential components.</p> <p>In stress testing, a 500-node cluster (3 masters, 497 workers) was created in under 11 minutes.</p>"},{"location":"#simplicity-that-scales","title":"Simplicity That Scales","text":"<ul> <li>No Terraform or Packer \u2014 a single CLI tool handles everything</li> <li>No management cluster \u2014 unlike Cluster API or Claudie, you don't need Kubernetes to create Kubernetes</li> <li>Simple YAML configuration \u2014 human-readable and version-controllable</li> <li>Idempotent operations \u2014 run <code>create</code> multiple times safely; it picks up where it left off</li> </ul>"},{"location":"#complete-control","title":"Complete Control","text":"<ul> <li>Your credentials stay local \u2014 the Hetzner API token never leaves your machine</li> <li>No third-party access \u2014 unlike managed services, no external party can access your clusters</li> <li>Open source (MIT License) \u2014 inspect, modify, and contribute to the code</li> <li>No recurring fees \u2014 you only pay Hetzner for infrastructure</li> </ul>"},{"location":"#production-ready-defaults","title":"Production-Ready Defaults","text":"<ul> <li>High availability \u2014 distribute masters and workers across locations</li> <li>Autoscaling \u2014 scale worker pools based on resource demands</li> <li>Private networking \u2014 cluster traffic stays off the public internet</li> <li>Automatic upgrades \u2014 the System Upgrade Controller handles rolling updates</li> </ul>"},{"location":"#documentation-structure","title":"Documentation Structure","text":""},{"location":"#getting-started_1","title":"Getting Started","text":"<ul> <li>Installation \u2014 Install hetzner-k3s on your system</li> <li>Creating a Cluster \u2014 Configuration reference and cluster creation</li> <li>Setting Up a Complete Stack \u2014 Ingress, TLS, and application deployment</li> </ul>"},{"location":"#operations","title":"Operations","text":"<ul> <li>Cluster Maintenance \u2014 Adding nodes, upgrades, and scaling</li> <li>Load Balancers \u2014 Configuring Hetzner load balancers</li> <li>Storage \u2014 Persistent volumes and storage options</li> <li>Deleting a Cluster \u2014 Clean removal of cluster resources</li> </ul>"},{"location":"#advanced-topics","title":"Advanced Topics","text":"<ul> <li>Recommendations \u2014 Best practices for different cluster sizes</li> <li>Large Clusters (100+ nodes) \u2014 Configuration for large-scale deployments</li> <li>Private Clusters \u2014 Clusters without public IPs</li> <li>Masters in Different Locations \u2014 Regional high availability</li> <li>Floating IP Egress \u2014 Consistent outbound IP addresses</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>Comparison with Other Tools \u2014 How hetzner-k3s compares to alternatives</li> <li>Troubleshooting \u2014 Common issues and solutions</li> <li>Upgrading from v1.x to v2.x \u2014 Migration guide</li> <li>Important Upgrade Notes \u2014 Version-specific considerations</li> </ul>"},{"location":"#community","title":"Community","text":"<ul> <li>Contributing and Support \u2014 How to contribute and get help</li> </ul>"},{"location":"#why-hetzner-cloud","title":"Why Hetzner Cloud?","text":"<p>Hetzner Cloud offers exceptional value for Kubernetes workloads:</p> <ul> <li>Up to 80% lower costs than AWS, Google Cloud, and Azure</li> <li>Transparent, all-inclusive pricing \u2014 traffic, IPv4/IPv6, DDoS protection, and firewalls included</li> <li>Six global locations \u2014 Germany (Nuremberg, Falkenstein), Finland (Helsinki), USA (Ashburn, Hillsboro), Singapore</li> <li>Flexible instance types \u2014 x86 and ARM architectures, including cost-effective ARM instances (CAX) for budget-friendly clusters</li> <li>25+ years of reliability \u2014 proven infrastructure trusted by companies worldwide</li> </ul>"},{"location":"#about-the-author","title":"About the Author","text":"<p>I'm Vito Botta, Lead Platform Architect at Brella, an event management platform based in Finland. I handle infrastructure, coding, and supporting the development team.</p> <p>I also spend time as a bug bounty hunter, finding and responsibly reporting security vulnerabilities.</p> <p>Connect with me at vitobotta.com. I'm available for consultancies around hetzner-k3s and Kubernetes on Hetzner.</p>"},{"location":"#why-sponsor","title":"Why Sponsor?","text":"<p>This project is maintained by a single developer in my spare time. Sponsorship helps me:</p> <ul> <li>Respond to issues faster</li> <li>Ship new features regularly</li> <li>Keep the project compatible with new Hetzner Cloud updates</li> </ul> <p>If hetzner-k3s saves you time or money, please consider supporting its development.</p>"},{"location":"#platinum-sponsors","title":"Platinum Sponsors","text":"<p>SprintPulse \u2014 the retrospective tool teams actually love. Run engaging retros with real-time collaboration, AI-powered insights, and actionable outcomes. Set up in 60 seconds, no training required.</p> <p>A huge thank you to Alamos GmbH for sponsoring the development of awesome features!</p> <p></p>"},{"location":"#backers","title":"Backers","text":"<p>Also thanks to @deubert-it, @jonasbadstuebner, @ricristian , @QuentinFAIDIDE for their support!</p>"},{"location":"#code-of-conduct","title":"Code of Conduct","text":"<p>Everyone interacting in the hetzner-k3s project's codebases, issue trackers, chat rooms and mailing lists is expected to follow the code of conduct.</p>"},{"location":"#license","title":"License","text":"<p>This tool is available as open source under the terms of the MIT License.</p>"},{"location":"Comparison_with_other_tools/","title":"Why hetzner-k3s Stands Out","text":"<p>There are several ways to run Kubernetes on Hetzner Cloud. This page explains why hetzner-k3s might be the right choice for your needs, with an honest look at how it compares to alternatives.</p>"},{"location":"Comparison_with_other_tools/#at-a-glance","title":"At a Glance","text":"Factor hetzner-k3s Managed Services Terraform-based Cluster API Setup time 2-3 minutes 5-10 minutes 15-30+ minutes 20+ minutes Dependencies CLI only Account signup Terraform, Packer, HCL Management cluster Data privacy Full control Third-party access Full control Full control Monthly cost Infrastructure only Infrastructure + platform fees (scale with cluster size) Infrastructure only Infrastructure only Credential exposure None API tokens to third party None None Learning curve Low Low Medium-High High Best for Most Hetzner users Zero-ops teams Terraform-native teams Multi-cloud standardization"},{"location":"Comparison_with_other_tools/#what-hetzner-k3s-offers","title":"What hetzner-k3s Offers","text":""},{"location":"Comparison_with_other_tools/#speed","title":"Speed","text":"<p>Creating a highly available cluster with 3 masters and 3 workers takes 2-3 minutes. This includes:</p> <ul> <li>Provisioning all infrastructure (instances, load balancer, private network, firewall)</li> <li>Deploying k3s to all nodes</li> <li>Installing Cloud Controller Manager, CSI driver, System Upgrade Controller, and Cluster Autoscaler</li> </ul> <p>In stress testing, a 500-node cluster was created in under 11 minutes.</p>"},{"location":"Comparison_with_other_tools/#minimal-dependencies","title":"Minimal Dependencies","text":"<p>You need exactly three things:</p> <ol> <li>A Hetzner Cloud API token</li> <li>An SSH key pair</li> <li>The hetzner-k3s CLI tool</li> </ol> <p>No Terraform, Packer, Ansible, or existing Kubernetes cluster. No need to learn HCL or manage Terraform state.</p>"},{"location":"Comparison_with_other_tools/#lightweight-kubernetes","title":"Lightweight Kubernetes","text":"<p>k3s uses significantly less memory and CPU than standard Kubernetes, leaving more resources for your workloads. It's a single binary, making deployments and upgrades fast and reliable.</p>"},{"location":"Comparison_with_other_tools/#full-data-sovereignty","title":"Full Data Sovereignty","text":"<ul> <li>Run the tool on your own machine</li> <li>Your Hetzner API token never leaves your system</li> <li>No third party gains access to your clusters, credentials, or data</li> <li>Complete control with no intermediary</li> </ul>"},{"location":"Comparison_with_other_tools/#cost-efficiency","title":"Cost Efficiency","text":"<ul> <li>The tool is free and open source</li> <li>You only pay for Hetzner infrastructure</li> <li>No per-user fees, per-cluster fees, or management fees</li> <li>No surprise charges as your team or infrastructure grows</li> </ul>"},{"location":"Comparison_with_other_tools/#how-alternatives-compare","title":"How Alternatives Compare","text":""},{"location":"Comparison_with_other_tools/#managed-services-cloudfleet-edka-syself","title":"Managed Services (Cloudfleet, Edka, Syself)","text":"<p>Managed services handle cluster operations for you, which is valuable if you want zero operational overhead.</p>"},{"location":"Comparison_with_other_tools/#considerations","title":"Considerations","text":"<p>Credential sharing: You provide your Hetzner API token to the service. The provider has ongoing access to your cloud account and can see your workloads. For regulated industries or security-conscious teams, this may be a concern.</p> <p>Pricing structure: Managed services typically charge: - A base cluster management fee - Per-vCPU fees for worker nodes - Sometimes per-user fees for enterprise tiers</p> <p>These platform fees add up quickly as clusters grow. A small development cluster might have modest fees, but production clusters with dozens of nodes can see platform costs that rival or exceed the underlying Hetzner infrastructure costs.</p> <p>For example, Cloudfleet's free tier limits you to 24 vCPUs with standard availability. Beyond that, costs scale with each additional vCPU. The difference between hetzner-k3s (infrastructure only) and managed services becomes substantial at scale\u2014potentially saving hundreds or thousands of euros per month on larger deployments.</p> <p>Vendor dependency: Your cluster management depends on the service's availability. If the provider changes pricing, terms, or discontinues service, you're affected. Migration requires effort.</p> <p>When managed services make sense: Teams that prioritize zero operational overhead over cost or data privacy. Organizations where someone else handling infrastructure is worth the ongoing fees.</p>"},{"location":"Comparison_with_other_tools/#terraform-based-solutions-terraform-hcloud-kube-hetzner-etc","title":"Terraform-based Solutions (terraform-hcloud-kube-hetzner, etc.)","text":"<p>Terraform-based solutions are powerful and flexible, especially for teams already using Terraform.</p>"},{"location":"Comparison_with_other_tools/#considerations_1","title":"Considerations","text":"<p>Multiple dependencies: You need: - Terraform or OpenTofu - Packer (for custom images) - kubectl CLI - hcloud CLI</p> <p>Each tool requires installation, configuration, and learning.</p> <p>Learning curve: You need to understand: - Terraform state management - HCL syntax - How changes propagate through Terraform plans - Debugging across multiple tools</p> <p>Ongoing maintenance: Terraform state must be stored and managed carefully. Upgrades require understanding how Terraform handles infrastructure drift.</p> <p>What you gain: More flexibility, infrastructure-as-code patterns familiar to platform teams, integration with existing Terraform workflows.</p> <p>When Terraform-based solutions make sense: Teams already using Terraform extensively. Organizations with platform engineering teams comfortable with IaC tooling.</p>"},{"location":"Comparison_with_other_tools/#claudie","title":"Claudie","text":"<p>Claudie is designed for multi-cloud and hybrid deployments. It provisions clusters across different cloud providers from a single management interface.</p>"},{"location":"Comparison_with_other_tools/#considerations_2","title":"Considerations","text":"<p>Requires a management cluster: Claudie runs on an existing Kubernetes cluster. You need Kubernetes to create Kubernetes.</p> <p>For production use, this management cluster needs to be resilient since Claudie maintains state for all clusters it provisions.</p> <p>Significant dependencies: The management cluster needs: - cert-manager - Multiple Claudie components (ansible, builder, claudie-operator, dynamodb, kube-eleven, kuber, minio, mongodb, scheduler, terraformer)</p> <p>Operational overhead: You're responsible for keeping the management cluster healthy. If it has issues, you can't manage your provisioned clusters.</p> <p>When Claudie makes sense: Organizations running clusters across multiple cloud providers who need unified management. Hybrid cloud scenarios where Hetzner is one of several providers.</p>"},{"location":"Comparison_with_other_tools/#talos-linux","title":"Talos Linux","text":"<p>Talos is an immutable, secure operating system built specifically for Kubernetes.</p>"},{"location":"Comparison_with_other_tools/#considerations_3","title":"Considerations","text":"<p>More complex setup: Deploying Talos on Hetzner requires: - Manually creating network components - Setting up a NAT gateway VM - Generating Talos-specific configuration files - Bootstrapping nodes individually</p> <p>Different operational model: Talos has no SSH access and is managed entirely via API. This provides enhanced security but requires adapting your workflows.</p> <p>When Talos makes sense: Organizations prioritizing immutable infrastructure and maximum security. Teams comfortable with the Talos operational model.</p>"},{"location":"Comparison_with_other_tools/#cluster-api-caph","title":"Cluster API (CAPH)","text":"<p>Cluster API provides declarative, Kubernetes-style management of cluster infrastructure.</p>"},{"location":"Comparison_with_other_tools/#considerations_4","title":"Considerations","text":"<p>Requires a management cluster: Like Claudie, you need an existing Kubernetes cluster (often a local kind cluster) to provision your workload cluster.</p> <p>Steeper learning curve: Understanding Cluster API concepts and CAPH-specific resources takes time.</p> <p>Kubernetes-native approach: If you're comfortable with Kubernetes operators and CRDs, this model may feel natural.</p> <p>When CAPH makes sense: Organizations standardizing on Cluster API across multiple providers. Teams that want to manage infrastructure with kubectl and GitOps.</p>"},{"location":"Comparison_with_other_tools/#real-world-considerations","title":"Real-World Considerations","text":""},{"location":"Comparison_with_other_tools/#development-and-testing","title":"Development and Testing","text":"<p>For quick iteration, hetzner-k3s excels. Creating and destroying clusters in minutes enables: - Ephemeral test environments - Rapid prototyping - Cost-effective experimentation (pay only for what you use)</p>"},{"location":"Comparison_with_other_tools/#small-to-medium-production","title":"Small to Medium Production","text":"<p>Most teams running 1-50 node clusters on Hetzner find hetzner-k3s sufficient. You get: - High availability with multi-location masters - Autoscaling for variable workloads - Zero recurring platform fees</p>"},{"location":"Comparison_with_other_tools/#large-scale-100-nodes","title":"Large Scale (100+ nodes)","text":"<p>hetzner-k3s has been tested with 500 nodes and is designed to scale beyond. Clusters over 100 nodes require some configuration changes\u2014see the Recommendations page for setup details.</p>"},{"location":"Comparison_with_other_tools/#multi-cloud-requirements","title":"Multi-Cloud Requirements","text":"<p>If you need clusters across AWS, GCP, and Hetzner with unified management, consider Claudie or Cluster API. hetzner-k3s is designed specifically for Hetzner.</p>"},{"location":"Comparison_with_other_tools/#the-bottom-line","title":"The Bottom Line","text":"<p>hetzner-k3s is designed for teams who want:</p> <ul> <li>Production-ready clusters in minutes, not hours</li> <li>Complete control over credentials and data</li> <li>No ongoing platform fees</li> <li>Minimal tooling complexity</li> </ul> <p>It's not the right choice if you:</p> <ul> <li>Want someone else to handle all operations (consider managed services)</li> <li>Need multi-cloud standardization (consider Cluster API or Claudie)</li> <li>Already have extensive Terraform infrastructure (consider terraform-hcloud-kube-hetzner)</li> </ul> <p>For most teams running Kubernetes on Hetzner Cloud, hetzner-k3s provides the best balance of speed, simplicity, and control.</p>"},{"location":"Contributing_and_support/","title":"Contributing and Support","text":"<p>hetzner-k3s is an open source project, and contributions are welcome!</p>"},{"location":"Contributing_and_support/#getting-help","title":"Getting Help","text":""},{"location":"Contributing_and_support/#github-issues","title":"GitHub Issues","text":"<p>If you're running into issues with the tool, please open an issue. Include:</p> <ul> <li>Your configuration file (with sensitive values redacted)</li> <li>Full output with debug mode enabled (<code>DEBUG=true hetzner-k3s ...</code>)</li> <li>Your operating system and hetzner-k3s version</li> <li>Steps to reproduce the issue</li> </ul>"},{"location":"Contributing_and_support/#github-discussions","title":"GitHub Discussions","text":"<p>For general questions, ideas, or discussions, use GitHub Discussions. This is a good place for:</p> <ul> <li>Questions about best practices</li> <li>Feature suggestions</li> <li>Sharing how you're using hetzner-k3s</li> </ul>"},{"location":"Contributing_and_support/#documentation","title":"Documentation","text":"<p>Check the Troubleshooting page for solutions to common issues.</p>"},{"location":"Contributing_and_support/#contributing-code","title":"Contributing Code","text":"<p>Pull requests are welcome! Whether it's fixing a bug, improving documentation, or adding a feature.</p>"},{"location":"Contributing_and_support/#development-environment","title":"Development Environment","text":"<p>hetzner-k3s is built using Crystal. You can develop using:</p> <ul> <li>VS Code with Dev Containers (recommended)</li> <li>Docker Compose</li> <li>Local Crystal installation</li> </ul>"},{"location":"Contributing_and_support/#vs-code-setup","title":"VS Code Setup","text":"<ol> <li>Install Visual Studio Code and the Dev Containers extension</li> <li>Open the project in VS Code (<code>code .</code> in the repository root)</li> <li>Click \"Reopen in Container\" when prompted</li> <li>Wait for the container to build</li> <li>Open a terminal inside the container</li> </ol> <p>Note: If you can't find the Dev Containers extension, ensure you're using the official VS Code build (some extensions are disabled in Open Source builds).</p>"},{"location":"Contributing_and_support/#docker-compose-setup","title":"Docker Compose Setup","text":"<p>If you prefer not to use VS Code:</p> <pre><code># Build and start the development container\ndocker compose up -d\n\n# Access the container\ndocker compose exec hetzner-k3s bash\n</code></pre>"},{"location":"Contributing_and_support/#running-the-tool","title":"Running the Tool","text":"<p>Inside the development container:</p> <pre><code># Run without building\ncrystal run ./src/hetzner-k3s.cr -- create --config cluster_config.yaml\n\n# Build a binary\ncrystal build ./src/hetzner-k3s.cr --static\n</code></pre> <p>The <code>--static</code> flag creates a statically linked binary that doesn't depend on external libraries.</p>"},{"location":"Contributing_and_support/#supporting-the-project","title":"Supporting the Project","text":"<p>If you or your company find this project useful, please consider becoming a sponsor. Your support helps:</p> <ul> <li>Fund ongoing development and maintenance</li> <li>Enable new features</li> <li>Keep the project actively maintained</li> </ul>"},{"location":"Contributing_and_support/#current-sponsors","title":"Current Sponsors","text":"<p>Platinum: Alamos GmbH</p> <p>Backers: @deubert-it, @jonasbadstuebner, @ricristian, @QuentinFAIDIDE</p>"},{"location":"Contributing_and_support/#consulting","title":"Consulting","text":"<p>Need help with hetzner-k3s, Kubernetes on Hetzner, or related infrastructure? The maintainer is available for consulting engagements.</p> <p>Contact: vitobotta.com</p>"},{"location":"Creating_a_cluster/","title":"Creating a Cluster","text":"<p>This page covers the full configuration reference for hetzner-k3s. For a quick start, see the Quick Start in the README or the Complete Tutorial.</p>"},{"location":"Creating_a_cluster/#configuration-file","title":"Configuration File","text":"<p>hetzner-k3s uses a YAML configuration file. Below is a complete example with all options. Commented lines are optional:</p> <pre><code>---\nhetzner_token: &lt;your token&gt;\ncluster_name: test\nkubeconfig_path: \"./kubeconfig\"\nk3s_version: v1.32.0+k3s1\n\nnetworking:\n  ssh:\n    port: 22\n    use_agent: false # set to true if your key has a passphrase\n    public_key_path: \"~/.ssh/id_ed25519.pub\"\n    private_key_path: \"~/.ssh/id_ed25519\"\n  allowed_networks:\n    ssh:\n      - 0.0.0.0/0\n    api: # this will firewall port 6443 on the nodes\n      - 0.0.0.0/0\n    # OPTIONAL: define extra inbound/outbound firewall rules.\n    # Each entry supports the following keys:\n    #   description (string, optional)\n    #   direction   (in | out, default: in)\n    #   protocol    (tcp | udp | icmp | esp | gre, default: tcp)\n    #   port        (single port \"80\", port range \"30000-32767\", or \"any\") \u2013 only relevant for tcp/udp\n    #   source_ips  (array of CIDR blocks) \u2013 required when direction is in\n    #   destination_ips (array of CIDR blocks) \u2013 required when direction is out\n    #\n    # IMPORTANT: Outbound traffic is allowed by default (implicit allow-all).\n    # If you add **any** outbound rule (direction: out), Hetzner Cloud switches\n    # the outbound chain to an implicit **deny-all**; only traffic matching your\n    # outbound rules will be permitted. Define outbound rules carefully to avoid\n    # accidentally blocking required egress (DNS, updates, etc.).\n    # NOTE: Hetzner Cloud Firewalls support **max 50 entries per firewall**. The built-\n    # in rules (SSH, ICMP, node-port ranges, etc.) use ~10 slots. If the sum of the\n    # default rules plus your custom ones exceeds 50, hetzner-k3s will abort with\n    # an error.\n    # custom_firewall_rules:\n    #   - description: \"Allow HTTP from any IPv4\"\n    #     direction: in\n    #     protocol: tcp\n    #     port: 80\n    #     source_ips:\n    #       - 0.0.0.0/0\n    #   - description: \"UDP game servers (outbound)\"\n    #     direction: out\n    #     protocol: udp\n    #     port: 60000-60100\n    #     destination_ips:\n    #       - 203.0.113.0/24\n  public_network:\n    ipv4: true\n    ipv6: true\n    # hetzner_ips_query_server_url: https://.. # for large clusters, see https://github.com/vitobotta/hetzner-k3s/blob/main/docs/Recommendations.md\n    # use_local_firewall: false # for large clusters, see https://github.com/vitobotta/hetzner-k3s/blob/main/docs/Recommendations.md\n  private_network:\n    enabled: true\n    subnet: 10.0.0.0/16\n    existing_network_name: \"\"\n  cni:\n    enabled: true\n    encryption: false\n    mode: flannel\n    cilium:\n      # Optional: specify a path to a custom values file for Cilium Helm chart\n      # When specified, this file will be used instead of the default values\n      # helm_values_path: \"./cilium-values.yaml\"\n      # chart_version: \"v1.17.2\"\n\n  # cluster_cidr: 10.244.0.0/16 # optional: a custom IPv4/IPv6 network CIDR to use for pod IPs\n  # service_cidr: 10.43.0.0/16 # optional: a custom IPv4/IPv6 network CIDR to use for service IPs. Warning, if you change this, you should also change cluster_dns!\n  # cluster_dns: 10.43.0.10 # optional: IPv4 Cluster IP for coredns service. Needs to be an address from the service_cidr range\n\ndatastore:\n  mode: etcd # etcd (default) or external\n  external_datastore_endpoint: postgres://....\n#  etcd:\n#    # etcd snapshot configuration (optional)\n#    snapshot_retention: 24\n#    snapshot_schedule_cron: \"0 * * * *\"\n#\n#    # S3 snapshot configuration (optional)\n#    s3_enabled: false\n#    s3_endpoint: \"\" # Can also be set with ETCD_S3_ENDPOINT environment variable\n#    s3_region: \"\" # Can also be set with ETCD_S3_REGION environment variable\n#    s3_bucket: \"\" # Can also be set with ETCD_S3_BUCKET environment variable\n#    s3_access_key: \"\" # Can also be set with ETCD_S3_ACCESS_KEY environment variable\n#    s3_secret_key: \"\" # Can also be set with ETCD_S3_SECRET_KEY environment variable\n#    s3_folder: \"\"\n#    s3_force_path_style: false\n\nschedule_workloads_on_masters: false # set to true to allow pods to be scheduled on master nodes (useful for small clusters)\n\n# image: rocky-9 # optional: default is ubuntu-24.04\n# autoscaling_image: 103908130 # optional, defaults to the `image` setting\n# snapshot_os: microos # optional: specified the os type when using a custom snapshot\n\nmasters_pool:\n  instance_type: cpx22\n  instance_count: 3 # for HA; you can also create a single master cluster for dev and testing (not recommended for production)\n  locations: # You can choose a single location for single master clusters or if you prefer to have all masters in the same location. For regional clusters (which are only available in the eu-central network zone), each master needs to be placed in a separate location.\n    - fsn1\n    - hel1\n    - nbg1\n\nworker_node_pools:\n- name: small-static\n  instance_type: cpx22\n  instance_count: 4\n  location: hel1\n  # image: debian-11\n  # labels: # Kubernetes labels to apply to nodes in this pool (for node selection in workloads)\n  #   - key: purpose\n  #     value: blah\n  # taints: # Kubernetes taints to apply to nodes in this pool (to repel pods unless they tolerate the taint)\n  #   - key: something\n  #     value: value1:NoSchedule\n- name: medium-autoscaled\n  instance_type: cpx32\n  location: fsn1\n  autoscaling:\n    enabled: true\n    min_instances: 0\n    max_instances: 3\n\n# addons:\n#   csi_driver:\n#     enabled: true   # Hetzner CSI driver (default true). Set to false to skip installation.\n#     manifest_url: \"https://raw.githubusercontent.com/hetznercloud/csi-driver/v2.18.3/deploy/kubernetes/hcloud-csi.yml\"\n#   traefik:\n#     enabled: false  # built-in Traefik ingress controller. Disabled by default.\n#   servicelb:\n#     enabled: false  # built-in ServiceLB. Disabled by default.\n#   metrics_server:\n#     enabled: false  # Kubernetes metrics-server addon. Disabled by default.\n#   cluster_autoscaler:\n#     enabled: true # Cluster Autoscaler addon (default true). Set to false to omit autoscaling.\n#     manifest_url: \"https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/hetzner/examples/cluster-autoscaler-run-on-master.yaml\"\n#     container_image_tag: \"v1.34.2\"\n#     scan_interval: \"10s\"                        # How often cluster is reevaluated for scale up or down\n#     scale_down_delay_after_add: \"10m\"           # How long after scale up that scale down evaluation resumes\n#     scale_down_delay_after_delete: \"10s\"        # How long after node deletion that scale down evaluation resumes\n#     scale_down_delay_after_failure: \"3m\"        # How long after scale down failure that scale down evaluation resumes\n#     max_node_provision_time: \"15m\"              # Maximum time CA waits for node to be provisioned\n#   cloud_controller_manager:\n#     enabled: true   # Hetzner Cloud Controller Manager (default true). Disabling stops automatic LB provisioning for Service objects.\n#     manifest_url: \"https://github.com/hetznercloud/hcloud-cloud-controller-manager/releases/download/v1.28.0/ccm-networks.yaml\"\n#   system_upgrade_controller:\n#     enabled: true   # System Upgrade Controller (default true). Set to false to omit autoscaling.\n#     deployment_manifest_url: \"https://github.com/rancher/system-upgrade-controller/releases/download/v0.18.0/system-upgrade-controller.yaml\"\n#     crd_manifest_url: \"https://github.com/rancher/system-upgrade-controller/releases/download/v0.18.0/crd.yaml\"\n#   embedded_registry_mirror:\n#     enabled: false # Enables fast p2p distribution of container images between nodes for faster pod startup. Check if your k3s version is compatible before enabling this option. You can find more information at https://docs.k3s.io/installation/registry-mirror\n\nprotect_against_deletion: true # prevents accidental deletion of the cluster with the \"hetzner-k3s delete\" command\n\ncreate_load_balancer_for_the_kubernetes_api: false # creates a load balancer for HA API access; note: Hetzner firewalls can't yet restrict access to load balancers by IP\n\nk3s_upgrade_concurrency: 1 # how many nodes to upgrade at the same time; increase for faster upgrades in large clusters, but higher values may impact availability\n\n# additional_packages:\n# - somepackage\n\n# additional_pre_k3s_commands:\n# - apt update\n# - apt upgrade -y\n\n# additional_post_k3s_commands:\n# - apt autoremove -y\n# For more advanced usage like resizing the root partition for use with Rook Ceph, see [Resizing root partition with additional post k3s commands](./Resizing_root_partition_with_post_create_commands.md)\n\n# kube_api_server_args:\n# - arg1\n# - ...\n# kube_scheduler_args:\n# - arg1\n# - ...\n# kube_controller_manager_args:\n# - arg1\n# - ...\n# kube_cloud_controller_manager_args:\n# - arg1\n# - ...\n# kubelet_args:\n# - arg1\n# - ...\n# kube_proxy_args:\n# - arg1\n# - ...\n# api_server_hostname: k8s.example.com # optional: DNS for the k8s API LoadBalancer. After the script has run, create a DNS record with the address of the API LoadBalancer.\n</code></pre> <p>Most settings are straightforward and easy to understand. To see a list of available k3s releases, you can run the command <code>hetzner-k3s releases</code>.</p> <p>If you prefer not to include the Hetzner token directly in the config file\u2014perhaps for use with CI or to safely commit the config to a repository\u2014you can use the <code>HCLOUD_TOKEN</code> environment variable instead. This variable takes precedence over the config file.</p> <p>When setting <code>masters_pool</code>.<code>instance_count</code>, keep in mind that if you set it to 1, the tool will create a control plane that is not highly available. For production clusters, it's better to set this to a number greater than 1. To avoid split brain issues with etcd, this number should be odd, and 3 is the recommended value. Additionally, for production environments, it's a good idea to configure masters in different locations using the <code>masters_pool</code>.<code>locations</code> setting.</p> <p>The <code>datastore</code> section configures how Kubernetes stores its cluster state. The default mode is <code>etcd</code>, which runs an embedded etcd cluster on your master nodes\u2014this is the recommended option for most deployments. For very large clusters or special requirements, you can use <code>external</code> mode with an external datastore (etcd, PostgreSQL, or MySQL) by specifying the connection string in <code>external_datastore_endpoint</code>. The etcd mode also supports optional S3 backup configuration for disaster recovery.</p> <p>You can define any number of worker node pools, either static or autoscaled, and create pools with nodes of different specifications to handle various workloads. Each pool can have optional <code>labels</code> and <code>taints</code>. Labels are key-value pairs that help you target specific nodes when scheduling workloads using node selectors or affinity rules. Taints prevent pods from being scheduled on certain nodes unless the pods explicitly tolerate the taint\u2014useful for dedicating nodes to specific workloads or keeping certain nodes free for particular purposes.</p> <p>Settings, such as <code>additional_packages</code>, <code>additional_pre_k3s_commands</code>, and <code>additional_post_k3s_commands</code>, can be specified at the root level of the configuration file or for each individual pool if different settings are needed. If these settings are configured at the pool level, they will override any settings defined at the root level.</p> <ul> <li><code>additional_pre_k3s_commands</code>: Commands executed before k3s installation</li> <li><code>additional_post_k3s_commands</code>: Commands executed after k3s is installed and configured</li> </ul> <p>For an example of using <code>additional_post_k3s_commands</code> to resize the root partition for use with storage solutions like Rook Ceph, see Resizing root partition with additional post k3s commands.</p> <p>The <code>addons</code> section controls which components hetzner-k3s installs automatically:</p> <ul> <li>csi_driver: The Hetzner CSI driver enables persistent volumes backed by Hetzner block storage. Enabled by default; disable if you're using alternative storage solutions like Rook Ceph or Longhorn.</li> <li>cloud_controller_manager: Integrates with Hetzner Cloud to provision load balancers automatically when you create Kubernetes Service objects of type LoadBalancer. Enabled by default.</li> <li>system_upgrade_controller: Enables zero-downtime rolling upgrades of k3s across your cluster. Enabled by default.</li> <li>cluster_autoscaler: Automatically scales worker node pools based on resource demands. Enabled by default when you have autoscaling pools defined.</li> <li>traefik: k3s's built-in ingress controller. Disabled by default; enable if you want a quick ingress solution without installing your own.</li> <li>servicelb: k3s's built-in load balancer for bare-metal environments. Disabled by default; typically not needed when using Hetzner's load balancers.</li> <li>metrics_server: Enables <code>kubectl top</code> commands for viewing resource usage. Disabled by default.</li> <li>embedded_registry_mirror: Enables peer-to-peer distribution of container images between nodes for faster pod startup. Disabled by default; check k3s version compatibility before enabling.</li> </ul> <p>The <code>api_server_hostname</code> setting is useful when you enable <code>create_load_balancer_for_the_kubernetes_api</code>. After the cluster is created, you can point this DNS name to the API load balancer's address, giving you a stable hostname for accessing the Kubernetes API.</p> <p>Currently, Hetzner Cloud offers six locations: two in Germany (<code>nbg1</code> in Nuremberg and <code>fsn1</code> in Falkenstein), one in Finland (<code>hel1</code> in Helsinki), two in the USA (<code>ash</code> in Ashburn, Virginia and <code>hil</code> in Hillsboro, Oregon), and one in Singapore (<code>sin</code>). Be aware that not all instance types are available in every location, so it\u2019s a good idea to check the Hetzner site and their status page for details.</p> <p>To explore the available instance types and their specifications, you can either check them manually when adding an instance within a project or run the following command with your Hetzner token:</p> <pre><code>curl -H \"Authorization: Bearer $API_TOKEN\" 'https://api.hetzner.cloud/v1/server_types'\n</code></pre> <p>To create the cluster run:</p> <pre><code>hetzner-k3s create --config cluster_config.yaml | tee create.log\n</code></pre> <p>This process will take a few minutes, depending on how many master and worker nodes you have.</p>"},{"location":"Creating_a_cluster/#disabling-public-ips-ipv4-or-ipv6-or-both-on-nodes","title":"Disabling public IPs (IPv4 or IPv6 or both) on nodes","text":"<p>To improve security and save on IPv4 address costs, you can disable the public interface for all nodes by setting <code>enable_public_net_ipv4: false</code> and <code>enable_public_net_ipv6: false</code>. These settings are global and will apply to all master and worker nodes. If you disable public IPs, make sure to run hetzner-k3s from a machine that has access to the same private network as the nodes, either directly or through a VPN.</p> <p>Additional networking setup is required via cloud-init, so it\u2019s important that the machine you use to run hetzner-k3s has internet access and DNS configured correctly. Otherwise, the cluster creation process will get stuck after creating the nodes. For more details and instructions, you can refer to this discussion.</p>"},{"location":"Creating_a_cluster/#using-alternative-os-images","title":"Using alternative OS images","text":"<p>By default, the image used for all nodes is <code>ubuntu-24.04</code>, but you can specify a different default image by using the root-level <code>image</code> config option. You can also set different images for different static node pools by using the <code>image</code> config option within each node pool. For example, if you have node pools with ARM instances, you can specify the correct OS image for ARM. To do this, set <code>image</code> to <code>103908130</code> with the specific image ID.</p> <p>However, for autoscaling, there\u2019s a current limitation in the Cluster Autoscaler for Hetzner. You can\u2019t specify different images for each autoscaled pool yet. For now, if you want to use a different image for all autoscaling pools, you can set the <code>autoscaling_image</code> option to override the default <code>image</code> setting.</p> <p>To see the list of available images, run the following:</p> <pre><code>export API_TOKEN=...\n\ncurl -H \"Authorization: Bearer $API_TOKEN\" 'https://api.hetzner.cloud/v1/images?per_page=100'\n</code></pre> <p>Besides the default OS images, you can also use a snapshot created from an existing instance. When using custom snapshots, make sure to specify the ID of the snapshot or image, not the description you assigned when creating the template instance.</p> <p>I\u2019ve tested snapshots with openSUSE MicroOS, but other options might work as well. You can easily create a MicroOS snapshot using this Terraform-based tool. The process only takes a few minutes. Once the snapshot is ready, you can use it with hetzner-k3s by setting the <code>image</code> configuration option to the ID of the snapshot and <code>snapshot_os</code> to <code>microos</code>.</p>"},{"location":"Creating_a_cluster/#keeping-a-project-per-cluster","title":"Keeping a Project per Cluster","text":"<p>If you plan to create multiple clusters within the same project, refer to the section on Configuring Cluster-CIDR and Service-CIDR. Ensure that each cluster has its own unique Cluster-CIDR and Service-CIDR. Overlapping ranges will cause issues. However, I still recommend separating clusters into different projects. This makes it easier to clean up resources\u2014if you want to delete a cluster, simply delete the entire project.</p>"},{"location":"Creating_a_cluster/#configuring-cluster-cidr-and-service-cidr","title":"Configuring Cluster-CIDR and Service-CIDR","text":"<p>Cluster-CIDR and Service-CIDR define the IP ranges used for pods and services, respectively. In most cases, you won\u2019t need to change these values. However, advanced setups might require adjustments to avoid network conflicts.</p> <p>Changing the Cluster-CIDR (Pod IP Range): To modify the Cluster-CIDR, uncomment or add the <code>cluster_cidr</code> option in your cluster configuration file and specify a valid CIDR notation for the network. Make sure this network is not a subnet of your private network.</p> <p>Changing the Service-CIDR (Service IP Range): To adjust the Service-CIDR, uncomment or add the <code>service_cidr</code> option in your configuration file and provide a valid CIDR notation. Again, ensure this network is not a subnet of your private network. Also, uncomment the <code>cluster_dns</code> option and provide a single IP address from the <code>service_cidr</code> range. This sets the IP address for the coredns service.</p> <p>Sizing the Networks: The networks you choose should have enough space for your expected number of pods and services. By default, <code>/16</code> networks are used. Select an appropriate size, as changing the CIDR later is not supported.</p>"},{"location":"Creating_a_cluster/#autoscaler-configuration","title":"Autoscaler Configuration","text":"<p>The cluster autoscaler automatically manages the number of worker nodes in your cluster based on resource demands. When you enable autoscaling for a worker node pool, you can also configure various timing parameters to fine-tune its behavior.</p>"},{"location":"Creating_a_cluster/#basic-autoscaling-configuration","title":"Basic Autoscaling Configuration","text":"<pre><code>worker_node_pools:\n- name: autoscaled-pool\n  instance_type: cpx32\n  location: fsn1\n  autoscaling:\n    enabled: true\n    min_instances: 1\n    max_instances: 10\n</code></pre>"},{"location":"Creating_a_cluster/#advanced-timing-configuration","title":"Advanced Timing Configuration","text":"<p>You can customize the autoscaler's behavior with these optional parameters at the root level of your configuration:</p> <pre><code>cluster_autoscaler:\n  scan_interval: \"2m\"                      # How often cluster is reevaluated for scale up or down\n  scale_down_delay_after_add: \"10m\"        # How long after scale up that scale down evaluation resumes\n  scale_down_delay_after_delete: \"10s\"     # How long after node deletion that scale down evaluation resumes\n  scale_down_delay_after_failure: \"15m\"    # How long after scale down failure that scale down evaluation resumes\n  max_node_provision_time: \"15m\"           # Maximum time CA waits for node to be provisioned\n\nworker_node_pools:\n- name: autoscaled-pool\n  instance_type: cpx32\n  location: fsn1\n  autoscaling:\n    enabled: true\n    min_instances: 1\n    max_instances: 10\n</code></pre>"},{"location":"Creating_a_cluster/#parameter-descriptions","title":"Parameter Descriptions","text":"<ul> <li><code>scan_interval</code>: Controls how frequently the cluster autoscaler evaluates whether scaling is needed. Shorter intervals mean faster response to load changes but more API calls.</li> <li> <p>Default: <code>10s</code></p> </li> <li> <p><code>scale_down_delay_after_add</code>: Prevents the autoscaler from immediately scaling down after adding nodes. This helps avoid thrashing when workloads are still starting up.</p> </li> <li> <p>Default: <code>10m</code></p> </li> <li> <p><code>scale_down_delay_after_delete</code>: Adds a delay before considering more scale-down operations after a node deletion. This ensures the cluster stabilizes before further changes.</p> </li> <li> <p>Default: <code>10s</code></p> </li> <li> <p><code>scale_down_delay_after_failure</code>: When a scale-down operation fails, this parameter controls how long to wait before attempting another scale-down.</p> </li> <li> <p>Default: <code>3m</code></p> </li> <li> <p><code>max_node_provision_time</code>: Sets the maximum time the autoscaler will wait for a new node to become ready. This is particularly useful for clusters with private networks where provisioning might take longer.</p> </li> <li>Default: <code>15m</code></li> </ul> <p>These settings apply globally to all autoscaling worker node pools in your cluster.</p>"},{"location":"Creating_a_cluster/#idempotency","title":"Idempotency","text":"<p>The <code>create</code> command can be run multiple times with the same configuration without causing issues, as the process is idempotent. If the process gets stuck or encounters errors (e.g., due to Hetzner API unavailability or timeouts), you can stop the command and rerun it with the same configuration to continue where it left off. Note that the kubeconfig will be overwritten each time you rerun the command.</p>"},{"location":"Creating_a_cluster/#limitations","title":"Limitations:","text":"<ul> <li>Using a snapshot instead of a default image will take longer to create instances compared to regular images.</li> <li>The <code>networking</code>.<code>allowed_networks</code>.<code>api</code> setting specifies which networks can access the Kubernetes API. This works with both single-master and multi-master clusters, but only when <code>create_load_balancer_for_the_kubernetes_api</code> is disabled. If the API load balancer is enabled, Hetzner's firewalls do not yet support load balancers, so the API would be exposed to the public internet regardless of the allowed networks configuration.</li> <li>If you enable autoscaling for a nodepool, avoid changing this setting later, as it can cause issues with the autoscaler.</li> <li>Autoscaling is only supported with Ubuntu or other default images, not snapshots.</li> <li>If you already have SSH keys in your Hetzner project, it's best to use a different key for your cluster\u2014unless there's already a key with the same name and fingerprint as the one in your config file.  Hetzner doesn't allow two keys with the same fingerprint in one project. So if you've already added the key from your config but under a different name, Hetzner won't let you add it again. In that case, hetzner-k3s will skip creating the key and won't inject any SSH key into the cluster nodes.  Without an SSH key, Hetzner sets up the nodes with password login instead. That means you'll get an email for each node with its root password. Managing several nodes this way can get tricky. To avoid this, just use a new keypair for your cluster\u2014unless the project already has a key that matches both the name and fingerprint in your config.</li> <li>SSH keys with passphrases can only be used if you set <code>networking</code>.<code>ssh</code>.<code>use_ssh_agent</code> to <code>true</code> and use an SSH agent to access your key. For example, on macOS, you can start an agent like this:</li> </ul> <pre><code>eval \"$(ssh-agent -s)\"\nssh-add --apple-use-keychain ~/.ssh/&lt;private key&gt;\n</code></pre>"},{"location":"Deleting_a_cluster/","title":"Deleting a Cluster","text":""},{"location":"Deleting_a_cluster/#basic-deletion","title":"Basic Deletion","text":"<p>To delete a cluster, you need to run the following command:</p> <pre><code>hetzner-k3s delete --config cluster_config.yaml\n</code></pre> <p>This command will remove all the resources in the Hetzner Cloud project that were created by <code>hetzner-k3s</code>.</p>"},{"location":"Deleting_a_cluster/#important-considerations","title":"Important Considerations","text":""},{"location":"Deleting_a_cluster/#protection-against-deletion","title":"Protection Against Deletion","text":"<p>Additionally, to delete a cluster, you must ensure that <code>protect_against_deletion</code> is set to <code>false</code>. When you execute the <code>delete</code> command, you'll also need to enter the cluster's name to confirm the deletion. These steps are in place to avoid accidentally deleting a cluster you intended to keep.</p>"},{"location":"Deleting_a_cluster/#resources-not-automatically-deleted","title":"Resources Not Automatically Deleted","text":"<p>Keep in mind that the following resources created by your applications will not be deleted automatically. You'll need to remove those manually:</p> <ul> <li>Load Balancers: Load balancers created by your applications (via Services of type LoadBalancer)</li> <li>Persistent Volumes: Persistent volumes and their underlying storage</li> <li>Floating IPs: Any floating IPs you've manually attached to instances</li> <li>Snapshots: Any snapshots created from instances</li> </ul> <p>This behavior is by design to prevent accidental data loss. These resources might be improved in future updates.</p>"},{"location":"Deleting_a_cluster/#manual-cleanup-steps","title":"Manual Cleanup Steps","text":""},{"location":"Deleting_a_cluster/#before-deleting-the-cluster","title":"Before Deleting the Cluster","text":"<ol> <li>Backup Important Data: Ensure you have backups of any important data stored in persistent volumes</li> <li>Export Application Configurations: Save any Kubernetes manifests, Helm values, or configurations you might need later</li> <li>Note Load Balancer IPs: If you have applications with public IPs, note them down as they might change if you recreate the cluster</li> </ol>"},{"location":"Deleting_a_cluster/#after-deleting-the-cluster","title":"After Deleting the Cluster","text":""},{"location":"Deleting_a_cluster/#manual-cleanup","title":"Manual Cleanup","text":"<p>You can easily delete any remaining resources using the Hetzner Cloud Console:</p> <ol> <li>Log in to your Hetzner Cloud Console</li> <li>Navigate to your project</li> <li>Delete remaining resources from the left sidebar:</li> <li>Load Balancers \u2192 Select and delete any application load balancers</li> <li>Volumes \u2192 Select and delete any persistent volumes</li> <li>Floating IPs \u2192 Select and delete any unused floating IPs</li> <li>Snapshots \u2192 Select and delete any unnecessary snapshots</li> </ol> <p>This visual approach is recommended as it's easier to identify which resources belong to your cluster and avoid accidental deletions.</p>"},{"location":"Deleting_a_cluster/#troubleshooting-deletion-issues","title":"Troubleshooting Deletion Issues","text":""},{"location":"Deleting_a_cluster/#cluster-still-protected","title":"Cluster Still Protected","text":"<p>If you get an error about the cluster being protected:</p> <ol> <li>Check Configuration: Ensure <code>protect_against_deletion: false</code> is set in your config file</li> <li>Verify Cluster Name: Make sure you're entering the correct cluster name when prompted</li> <li>Check kubeconfig: Sometimes the cluster name is read from the kubeconfig location</li> </ol>"},{"location":"Deleting_a_cluster/#resources-stuck-in-deletion","title":"Resources Stuck in Deletion","text":"<p>If some resources are stuck and not being deleted:</p> <ol> <li>Check Hetzner Console: Log in to the Hetzner Cloud Console to see the current state</li> <li>Wait and Retry: Sometimes there's a delay in API updates, wait a few minutes and retry</li> </ol>"},{"location":"Deleting_a_cluster/#network-resources-not-deleted","title":"Network Resources Not Deleted","text":"<p>If networks, firewall rules, or other network resources remain:</p> <ol> <li>Check Dependencies: Make sure no instances are still using the network, load balancer or firewall</li> <li>Delete Manually: Use the Hetzner Console or API to clean up remaining network resources</li> </ol>"},{"location":"Deleting_a_cluster/#alternative-delete-entire-project","title":"Alternative: Delete Entire Project","text":"<p>If your cluster is the only thing in the Hetzner Cloud project, you might find it easier to delete the entire project instead:</p> <ol> <li>Go to Hetzner Cloud Console</li> <li>Navigate to your project</li> <li>Click on \"Settings\"</li> <li>Select \"Delete Project\"</li> </ol> <p>This will delete everything in the project, including any resources you might have forgotten about.</p> <p>Warning: This is irreversible! Only do this if you're certain you don't need anything in the project.</p>"},{"location":"Deleting_a_cluster/#best-practices","title":"Best Practices","text":""},{"location":"Deleting_a_cluster/#planning-for-deletion","title":"Planning for Deletion","text":"<p>When setting up your cluster, consider:</p> <ol> <li>Use Projects Wisely: Consider creating separate projects for different clusters or environments</li> <li>Document Dependencies: Keep track of external resources that depend on your cluster</li> </ol>"},{"location":"Deleting_a_cluster/#post-deletion-checklist","title":"Post-Deletion Checklist","text":"<p>After deleting your cluster, verify:</p> <ul> <li> No instances are running</li> <li> No load balancers are active</li> <li> No volumes are attached</li> <li> No floating IPs are allocated</li> <li> Network usage has stopped</li> <li> Billing reflects the changes</li> </ul>"},{"location":"Deleting_a_cluster/#cost-monitoring","title":"Cost Monitoring","text":"<p>Monitor your Hetzner Cloud billing dashboard for a few days after deletion to ensure:</p> <ul> <li>No unexpected charges appear</li> <li>All compute resources have been properly terminated</li> <li>Network and storage costs stop accumulating</li> </ul> <p>If you see unexpected charges, check for orphaned resources that might need manual cleanup.</p>"},{"location":"Floating_IP_egress/","title":"Floating IP Egress","text":"<p>This guide explains how to configure a dedicated egress IP address for all outbound traffic from your cluster. This is useful when external services need to allowlist your cluster's IP address, or when you need a consistent source IP for outgoing connections.</p> <p>This setup uses Cilium's egress gateway feature with a Hetzner floating IP.</p>"},{"location":"Floating_IP_egress/#enable-cilium-egress-gateway","title":"Enable Cilium Egress Gateway","text":"<p>In your cluster configuration, enable Cilium with the egress gateway feature:</p> <pre><code>networking:\n  cni:\n    enabled: true\n    mode: cilium\n    cilium_egress_gateway: true\n</code></pre>"},{"location":"Floating_IP_egress/#create-a-dedicated-egress-node-pool","title":"Create a Dedicated Egress Node Pool","text":"<p>Add a worker node pool that will serve as the egress gateway. This node will route all outbound traffic through the floating IP:</p> <pre><code>worker_node_pools:\n  - name: egress\n    instance_type: cax21\n    location: hel1\n    instance_count: 1\n    autoscaling:\n      enabled: false\n    labels:\n      - key: node.kubernetes.io/role\n        value: \"egress\"\n    taints:\n      - key: node.kubernetes.io/role\n        value: egress:NoSchedule\n</code></pre> <p>The taint ensures that regular workloads are not scheduled on this node\u2014it's dedicated to handling egress traffic.</p>"},{"location":"Floating_IP_egress/#assign-a-floating-ip","title":"Assign a Floating IP","text":"<ol> <li>Create a floating IP in the Hetzner Cloud Console (or via API/CLI) in the same location as your egress node</li> <li>Assign the floating IP to the egress node</li> <li>Configure the floating IP on the node's network interface (this may require additional cloud-init commands or manual configuration)</li> </ol>"},{"location":"Floating_IP_egress/#apply-the-egress-gateway-policy","title":"Apply the Egress Gateway Policy","text":"<p>Create a <code>CiliumEgressGatewayPolicy</code> to route outbound traffic through the egress node:</p> <pre><code>apiVersion: cilium.io/v2\nkind: CiliumEgressGatewayPolicy\nmetadata:\n  name: egress-global\nspec:\n  selectors:\n    - podSelector: {}\n\n  destinationCIDRs:\n    - \"0.0.0.0/0\"\n  excludedCIDRs:\n    - \"10.0.0.0/8\"\n\n  egressGateway:\n    nodeSelector:\n      matchLabels:\n        node.kubernetes.io/role: egress\n    egressIP: YOUR_FLOATING_IP\n</code></pre> <p>Replace <code>YOUR_FLOATING_IP</code> with your actual floating IP address.</p> <p>Apply the policy:</p> <pre><code>kubectl apply -f egress-policy.yaml\n</code></pre>"},{"location":"Floating_IP_egress/#how-it-works","title":"How It Works","text":"<ul> <li><code>selectors</code>: Matches all pods (empty <code>podSelector</code> means all pods in the cluster)</li> <li><code>destinationCIDRs</code>: Routes all external traffic (<code>0.0.0.0/0</code>) through the egress gateway</li> <li><code>excludedCIDRs</code>: Excludes internal/private network traffic (<code>10.0.0.0/8</code>) from being routed through the gateway, allowing pod-to-pod and pod-to-service communication to work normally</li> <li><code>egressGateway</code>: Specifies which node handles the egress traffic and what source IP to use</li> </ul> <p>Once configured, all outbound traffic from your cluster to external destinations will originate from your floating IP address.</p>"},{"location":"Important_upgrade_notes/","title":"Important Upgrade Notes","text":""},{"location":"Important_upgrade_notes/#openssh-upgrade-notice-friday-august-1-2025","title":"OpenSSH Upgrade Notice - Friday, August 1, 2025","text":""},{"location":"Important_upgrade_notes/#critical-information","title":"Critical Information","text":"<p>Due to a recent OpenSSH upgrade made available for Ubuntu, there is a significant risk that cluster nodes created with a version of hetzner-k3s prior to 2.3.4 might become unreachable via SSH once OpenSSH gets upgraded and the nodes are rebooted.</p>"},{"location":"Important_upgrade_notes/#the-problem","title":"The Problem","text":"<p>The OpenSSH upgrade changes systemd socket configuration behavior, which can cause SSH connectivity issues if the socket configuration file <code>/etc/systemd/system/ssh.socket.d/listen.conf</code> is not properly configured to handle IPv6 binding.</p>"},{"location":"Important_upgrade_notes/#solution-for-reachable-nodes","title":"Solution for Reachable Nodes","text":"<p>If the nodes in your cluster are still reachable via SSH, you can fix this issue by running the following command:</p> <pre><code>hetzner-k3s run --config &lt;your-config-file&gt; --script fix-ssh.sh\n</code></pre> <p>This command will automatically fix the contents of <code>/etc/systemd/system/ssh.socket.d/listen.conf</code> to ensure SSH connectivity continues working after the OpenSSH server upgrade.</p> <p>The script is available at the root of this project's repository and will: - Create a backup of the original configuration file with a timestamp - Properly configure the socket file to handle both IPv4 and IPv6 connections - Preserve all existing <code>ListenStream</code> configurations - Restart the SSH socket to apply changes</p>"},{"location":"Important_upgrade_notes/#workaround-for-unreachable-nodes","title":"Workaround for Unreachable Nodes","text":"<p>If your nodes are no longer reachable via SSH due to OpenSSH already having been upgraded, there is a manual workaround available:</p> <ol> <li>Run the <code>kube-shell</code> script from the project's repository (in the <code>bin</code> directory)</li> <li>Specify the name of a node to fix as the first and only argument</li> <li>This will open an SSH-like session on the node via <code>kubectl</code> using a temporary privileged pod</li> <li>Within this session, manually modify <code>/etc/systemd/system/ssh.socket.d/listen.conf</code> to append the line:    <pre><code>BindIPv6Only=default\n</code></pre></li> </ol> <p>Important: This manual method must be performed for each node individually Exercise caution when modifying system configuration files.</p>"},{"location":"Important_upgrade_notes/#affected-versions","title":"Affected Versions","text":"<ul> <li>Fixed in: hetzner-k3s 2.3.5 and later</li> <li>Affected: All versions prior to 2.3.5</li> </ul>"},{"location":"Important_upgrade_notes/#recommendation","title":"Recommendation","text":"<p>We strongly recommend upgrading to hetzner-k3s 2.3.5 or later and running the fix script proactively before any OpenSSH upgrades occur to prevent any connectivity issues.</p>"},{"location":"Important_upgrade_notes/#additional-resources","title":"Additional Resources","text":"<p>For more information about SSH configuration and troubleshooting, please refer to the Troubleshooting documentation.</p>"},{"location":"Installation/","title":"Installation","text":"<p>Get hetzner-k3s running on your system in under a minute.</p>"},{"location":"Installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing hetzner-k3s, you'll need:</p> Requirement Description Hetzner Cloud account Sign up here if you don't have one API token Create one in Cloud Console \u2192 Security \u2192 API Tokens (read &amp; write permissions) SSH key pair For accessing cluster nodes kubectl For interacting with your cluster (installation guide) Helm For installing applications (installation guide)"},{"location":"Installation/#macos","title":"macOS","text":""},{"location":"Installation/#homebrew-recommended","title":"Homebrew (Recommended)","text":"<pre><code>brew install vitobotta/tap/hetzner_k3s\n</code></pre> <p>Homebrew also works on Linux \u2014 see the Linux section below.</p>"},{"location":"Installation/#binary-installation","title":"Binary Installation","text":"<p>If you prefer not to use Homebrew, install the required dependencies first:</p> <ul> <li>libevent</li> <li>bdw-gc</li> <li>libyaml</li> <li>pcre</li> <li>gmp</li> </ul> <p>Apple Silicon: <pre><code>wget https://github.com/vitobotta/hetzner-k3s/releases/download/v2.4.5/hetzner-k3s-macos-arm64\nchmod +x hetzner-k3s-macos-arm64\nsudo mv hetzner-k3s-macos-arm64 /usr/local/bin/hetzner-k3s\n</code></pre></p> <p>Intel: <pre><code>wget https://github.com/vitobotta/hetzner-k3s/releases/download/v2.4.5/hetzner-k3s-macos-amd64\nchmod +x hetzner-k3s-macos-amd64\nsudo mv hetzner-k3s-macos-amd64 /usr/local/bin/hetzner-k3s\n</code></pre></p>"},{"location":"Installation/#linux","title":"Linux","text":""},{"location":"Installation/#homebrew-recommended_1","title":"Homebrew (Recommended)","text":"<p>Homebrew works on Linux as well as macOS.</p> <pre><code>brew install vitobotta/tap/hetzner_k3s\n</code></pre>"},{"location":"Installation/#amd64-x86_64","title":"amd64 (x86_64)","text":"<pre><code>wget https://github.com/vitobotta/hetzner-k3s/releases/download/v2.4.5/hetzner-k3s-linux-amd64\nchmod +x hetzner-k3s-linux-amd64\nsudo mv hetzner-k3s-linux-amd64 /usr/local/bin/hetzner-k3s\n</code></pre>"},{"location":"Installation/#arm64-arm","title":"arm64 (ARM)","text":"<pre><code>wget https://github.com/vitobotta/hetzner-k3s/releases/download/v2.4.5/hetzner-k3s-linux-arm64\nchmod +x hetzner-k3s-linux-arm64\nsudo mv hetzner-k3s-linux-arm64 /usr/local/bin/hetzner-k3s\n</code></pre>"},{"location":"Installation/#fedora-and-similar-distributions","title":"Fedora and Similar Distributions","text":"<p>Some distributions (like Fedora) may have OpenSSL compatibility issues. If you encounter errors, set these environment variables before running hetzner-k3s:</p> <pre><code>export OPENSSL_CONF=/dev/null\nexport OPENSSL_MODULES=/dev/null\n</code></pre> <p>For convenience, add a wrapper function to your <code>~/.bashrc</code> or <code>~/.zshrc</code>:</p> <pre><code>hetzner-k3s() {\n    OPENSSL_CONF=/dev/null OPENSSL_MODULES=/dev/null command hetzner-k3s \"$@\"\n}\n</code></pre>"},{"location":"Installation/#windows","title":"Windows","text":"<p>Use the Linux binary with WSL (Windows Subsystem for Linux).</p> <p>After installing WSL, follow the Linux installation instructions above.</p>"},{"location":"Installation/#verify-installation","title":"Verify Installation","text":"<p>Check that hetzner-k3s is installed correctly:</p> <pre><code>hetzner-k3s --version\n</code></pre> <p>You should see the version number displayed.</p>"},{"location":"Installation/#next-steps","title":"Next Steps","text":"<p>Now that hetzner-k3s is installed:</p> <ol> <li>Create your first cluster \u2014 Configuration reference and detailed options</li> <li>Set up a complete stack \u2014 Tutorial with ingress, TLS, and a sample application</li> </ol>"},{"location":"Installation/#updating","title":"Updating","text":"<p>To update to the latest version:</p> <p>Homebrew: <pre><code>brew upgrade vitobotta/tap/hetzner_k3s\n</code></pre></p> <p>Binary installations: Download and replace the binary using the same steps as the initial installation.</p> <p>Check the releases page for the latest version and changelog.</p>"},{"location":"Load_balancers/","title":"Load Balancers","text":"<p>Hetnzer-k3s automatically installs and configures the Hetzner Cloud Controller Manager, which enables you to create and manage Hetzner Load Balancers directly from Kubernetes using Services of type <code>LoadBalancer</code>.</p>"},{"location":"Load_balancers/#overview","title":"Overview","text":"<p>When you create a Service of type <code>LoadBalancer</code> in your cluster, the Cloud Controller Manager will automatically:</p> <ol> <li>Create a new Hetzner Load Balancer</li> <li>Configure it with the specified settings and annotations</li> <li>Set up health checks for your pods</li> <li>Update the Service status with the Load Balancer's external IP</li> </ol>"},{"location":"Load_balancers/#basic-configuration","title":"Basic Configuration","text":""},{"location":"Load_balancers/#essential-annotations","title":"Essential Annotations","text":"<p>At a minimum, you'll need these two annotations:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-app-service\n  annotations:\n    load-balancer.hetzner.cloud/location: nbg1  # Ensures the load balancer is in the same network zone as your nodes\n    load-balancer.hetzner.cloud/use-private-ip: \"true\"  # Routes traffic between the load balancer and nodes through the private network\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n</code></pre>"},{"location":"Load_balancers/#recommended-annotations","title":"Recommended Annotations","text":"<p>While the above are essential, I also recommend adding these annotations for production use:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-app-service\n  annotations:\n    # Basic configuration\n    load-balancer.hetzner.cloud/location: nbg1\n    load-balancer.hetzner.cloud/use-private-ip: \"true\"\n\n    # Additional recommended settings\n    load-balancer.hetzner.cloud/hostname: app.example.com  # Custom hostname for the load balancer\n    load-balancer.hetzner.cloud/name: my-app-lb           # Custom name for the load balancer\n    load-balancer.hetzner.cloud/http-redirect-https: 'false'  # Disable HTTP to HTTPS redirect (handled by ingress)\n    load-balancer.hetzner.cloud/uses-proxyprotocol: 'true'    # Enable proxy protocol to preserve client IP\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n</code></pre>"},{"location":"Load_balancers/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"Load_balancers/#proxy-protocol-and-client-ip-preservation","title":"Proxy Protocol and Client IP Preservation","text":"<p>The proxy protocol is important because it allows your ingress controller and applications to detect the real client IP address. </p> <pre><code>annotations:\n  load-balancer.hetzner.cloud/uses-proxyprotocol: 'true'\n  load-balancer.hetzner.cloud/hostname: app.example.com\n</code></pre> <p>Why use hostname with proxy protocol?</p> <p>Enabling the proxy protocol can cause issues with cert-manager failing HTTP-01 challenges. To fix this, Hetzner and other providers recommend using a hostname instead of an IP for the load balancer. For more details, read this explanation.</p>"},{"location":"Load_balancers/#multiple-services-and-ports","title":"Multiple Services and Ports","text":"<p>You can expose multiple ports and services through the same load balancer:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: multi-port-app\n  annotations:\n    load-balancer.hetzner.cloud/location: nbg1\n    load-balancer.hetzner.cloud/use-private-ip: \"true\"\n    load-balancer.hetzner.cloud/uses-proxyprotocol: 'true'\nspec:\n  type: LoadBalancer\n  selector:\n    app: multi-port-app\n  ports:\n    - protocol: TCP\n      name: http\n      port: 80\n      targetPort: 8080\n    - protocol: TCP\n      name: https\n      port: 443\n      targetPort: 8443\n</code></pre>"},{"location":"Load_balancers/#load-balancer-algorithm","title":"Load Balancer Algorithm","text":"<p>You can specify the load balancing algorithm:</p> <pre><code>annotations:\n  load-balancer.hetzner.cloud/algorithm: round_robin  # Options: round_robin, least_connections\n</code></pre>"},{"location":"Load_balancers/#health-checks","title":"Health Checks","text":"<p>Customize health check behavior:</p> <pre><code>annotations:\n  load-balancer.hetzner.cloud/health-check-interval: \"15s\"\n  load-balancer.hetzner.cloud/health-check-timeout: \"10s\"\n  load-balancer.hetzner.cloud/health-check-retries: \"3\"\n</code></pre>"},{"location":"Load_balancers/#example-nginx-ingress-controller","title":"Example: NGINX Ingress Controller","text":"<p>Here's a complete example for deploying NGINX Ingress Controller with a Load Balancer:</p> <pre><code>---\n# ingress-nginx-values.yaml\ncontroller:\n  kind: DaemonSet\n  service:\n    annotations:\n      # Set the location (must match your node locations)\n      load-balancer.hetzner.cloud/location: nbg1\n\n      # Load balancer name\n      load-balancer.hetzner.cloud/name: nginx-ingress-lb\n\n      # Use private network for internal communication\n      load-balancer.hetzner.cloud/use-private-ip: \"true\"\n\n      # Enable proxy protocol to preserve client IPs\n      load-balancer.hetzner.cloud/uses-proxyprotocol: 'true'\n\n      # Set hostname for the load balancer (replace with your domain)\n      load-balancer.hetzner.cloud/hostname: ingress.example.com\n\n      # Disable automatic HTTP to HTTPS redirect (handled by ingress)\n      load-balancer.hetzner.cloud/http-redirect-https: 'false'\n</code></pre> <p>Install with Helm:</p> <pre><code># Add the Helm repository\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\n\n# Install ingress-nginx with custom annotations\nhelm upgrade --install \\\n  ingress-nginx ingress-nginx/ingress-nginx \\\n  --namespace ingress-nginx \\\n  --create-namespace \\\n  -f ingress-nginx-values.yaml\n</code></pre>"},{"location":"Load_balancers/#available-locations","title":"Available Locations","text":"<p>Hetzner Cloud has data centers in these locations:</p> Location Code City Country <code>nbg1</code> Nuremberg Germany <code>fsn1</code> Falkenstein Germany <code>hel1</code> Helsinki Finland <code>ash</code> Ashburn, VA USA <code>hil</code> Hillsboro, OR USA <code>sin</code> Singapore Singapore <p>Make sure to choose a location where your nodes are deployed or across multiple locations if you have a distributed setup.</p>"},{"location":"Load_balancers/#complete-list-of-annotations","title":"Complete List of Annotations","text":"<p>For a full list of available annotations and their descriptions, refer to the official documentation.</p> <p>Common annotations include:</p> Annotation Description Default <code>load-balancer.hetzner.cloud/location</code> Location of the load balancer Required <code>load-balancer.hetzner.cloud/use-private-ip</code> Use private network for node communication <code>false</code> <code>load-balancer.hetzner.cloud/hostname</code> Custom hostname for the load balancer Auto-generated <code>load-balancer.hetzner.cloud/name</code> Custom name for the load balancer Service name <code>load-balancer.hetzner.cloud/uses-proxyprotocol</code> Enable proxy protocol <code>false</code> <code>load-balancer.hetzner.cloud/algorithm</code> Load balancing algorithm <code>round_robin</code> <code>load-balancer.hetzner.cloud/http-redirect-https</code> Enable HTTP to HTTPS redirect <code>false</code> <code>load-balancer.hetzner.cloud/health-check-interval</code> Health check interval <code>15s</code> <code>load-balancer.hetzner.cloud/health-check-timeout</code> Health check timeout <code>10s</code> <code>load-balancer.hetzner.cloud/health-check-retries</code> Health check retry count <code>3</code>"},{"location":"Load_balancers/#troubleshooting","title":"Troubleshooting","text":""},{"location":"Load_balancers/#load-balancer-stuck-in-pending","title":"Load Balancer Stuck in \"Pending\"","text":"<p>If your load balancer fails to get an external IP:</p> <ol> <li>Check Annotations: Ensure all required annotations are set correctly</li> <li>Verify Location: Make sure the specified location exists and has capacity</li> <li>Check Logs: Examine the Cloud Controller Manager logs:    <pre><code>kubectl logs -n kube-system -l k8s-app=hcloud-cloud-controller-manager\n</code></pre></li> <li>Check Service: Verify the Service definition is correct:    <pre><code>kubectl describe service &lt;service-name&gt;\n</code></pre></li> </ol>"},{"location":"Load_balancers/#health-check-failures","title":"Health Check Failures","text":"<p>If health checks are failing:</p> <ol> <li>Check Pod Status: Ensure pods are running and ready</li> <li>Verify Port Mapping: Confirm the targetPort matches your application port</li> <li>Check Network Policies: Ensure no network policies are blocking traffic</li> <li>Review Pod Logs: Check application logs for errors</li> </ol>"},{"location":"Load_balancers/#connection-issues","title":"Connection Issues","text":"<p>If you can't connect through the load balancer:</p> <ol> <li>Check Security Groups: Verify firewall rules allow traffic</li> <li>Test Direct Access: Try accessing pods directly to isolate the issue</li> <li>Check DNS: If using a hostname, ensure DNS resolves correctly</li> <li>Monitor Traffic: Use <code>kubectl logs</code> and <code>kubectl describe</code> to trace traffic flow</li> </ol>"},{"location":"Load_balancers/#best-practices","title":"Best Practices","text":"<ol> <li>Use Private Networks: Always set <code>use-private-ip: \"true\"</code> for better security and performance</li> <li>Enable Proxy Protocol: Use <code>uses-proxyprotocol: 'true'</code> to preserve client IP addresses</li> <li>Choose Right Location: Place load balancers close to your users for lower latency</li> <li>Monitor Health: Regularly check load balancer health and metrics</li> <li>Use Meaningful Names: Set custom names for easier identification in the Hetzner console</li> <li>Configure DNS: Set up proper DNS records for your load balancer hostnames</li> </ol>"},{"location":"Maintenance/","title":"Maintenance","text":""},{"location":"Maintenance/#adding-nodes","title":"Adding Nodes","text":"<p>To add one or more nodes to a node pool, simply update the instance count in the configuration file for that node pool and run the <code>create</code> command again.</p>"},{"location":"Maintenance/#scaling-down-a-node-pool","title":"Scaling Down a Node Pool","text":"<p>To reduce the size of a node pool:</p> <ol> <li>Lower the instance count in the configuration file to ensure the extra nodes are not recreated in the future.</li> <li>Drain and delete the additional nodes from Kubernetes. These are typically the last nodes when sorted alphabetically by name (<code>kubectl drain Node</code> followed by <code>kubectl delete node &lt;name&gt;</code>).</li> <li>Remove the corresponding instances from the cloud console if the Cloud Controller Manager doesn\u2019t handle this automatically. Make sure you delete the correct ones!</li> </ol>"},{"location":"Maintenance/#replacing-a-problematic-node","title":"Replacing a Problematic Node","text":"<ol> <li>Drain and delete the node from Kubernetes (<code>kubectl drain &lt;name&gt;</code> followed by <code>kubectl delete node &lt;name&gt;</code>).</li> <li>Delete the correct instance from the cloud console.</li> <li>Run the <code>create</code> command again. This will recreate the missing node and add it to the cluster.</li> </ol>"},{"location":"Maintenance/#converting-a-non-ha-cluster-to-ha","title":"Converting a Non-HA Cluster to HA","text":"<p>Converting a single-master, non-HA cluster to a multi-master HA cluster is straightforward. Increase the masters instance count and rerun the <code>create</code> command. This will set up a load balancer for the API server (if enabled) and update the kubeconfig to direct API requests through the load balancer or one of the master contexts. For production clusters, it\u2019s also a good idea to place the masters in different locations (refer to this page for more details).</p>"},{"location":"Maintenance/#upgrading-to-a-new-version-of-k3s","title":"Upgrading to a New Version of k3s","text":""},{"location":"Maintenance/#step-1-initiate-the-upgrade","title":"Step 1: Initiate the Upgrade","text":"<p>For the first upgrade of your cluster, simply run the following command to update to a newer version of k3s:</p> <pre><code>hetzner-k3s upgrade --config cluster_config.yaml --new-k3s-version v1.27.1-rc2+k3s1\n</code></pre> <p>Specify the new k3s version as an additional parameter, and the configuration file will be updated automatically during the upgrade. To view available k3s releases, run the command <code>hetzner-k3s releases</code>.</p> <p>Note: For single-master clusters, the API server will be briefly unavailable during the control plane upgrade.</p>"},{"location":"Maintenance/#step-2-monitor-the-upgrade-process","title":"Step 2: Monitor the Upgrade Process","text":"<p>After running the upgrade command, you must monitor the upgrade jobs in the <code>system-upgrade</code> namespace to ensure all nodes are successfully upgraded:</p> <pre><code># Watch the upgrade progress for all nodes\nwatch kubectl get nodes -owide\n\n# Monitor upgrade jobs and plans\nwatch kubectl get jobs,pods -n system-upgrade\n\n# Check upgrade plans status\nkubectl get plan -n system-upgrade -o wide\n\n# Check upgrade job logs\nkubectl logs -n system-upgrade -f job/&lt;upgrade-job-name&gt;\n</code></pre> <p>You will see the masters upgrading one at a time, followed by the worker nodes. The upgrade process creates upgrade jobs in the <code>system-upgrade</code> namespace that handle the actual node upgrades.</p>"},{"location":"Maintenance/#step-3-verify-upgrade-completion","title":"Step 3: Verify Upgrade Completion","text":"<p>Before proceeding, ensure all upgrade jobs have completed successfully and all nodes are running the new k3s version:</p> <pre><code># Check that all upgrade jobs are completed\nkubectl get jobs -n system-upgrade\n\n# Verify all nodes are ready and running the new version\nkubectl get nodes -o wide\n\n# Check for any failed or pending jobs\nkubectl get jobs -n system-upgrade --field-selector status.failed=1\nkubectl get jobs -n system-upgrade --field-selector status.active=1\n</code></pre> <p>\u2705 Upgrade Completion Checklist:</p> <ul> <li> All upgrade jobs in <code>system-upgrade</code> namespace have completed</li> <li> All nodes show <code>Ready</code> status</li> <li> All nodes display the new k3s version in <code>kubectl get nodes -owide</code></li> <li> No active or failed upgrade jobs remain</li> </ul>"},{"location":"Maintenance/#step-4-run-create-command-critical","title":"Step 4: Run Create Command (CRITICAL)","text":"<p>Once all upgrade jobs have completed and all nodes have been successfully updated to the new k3s version, you MUST run the create command:</p> <pre><code>hetzner-k3s create --config cluster_config.yaml\n</code></pre>"},{"location":"Maintenance/#why-this-step-is-essential","title":"Why This Step is Essential:","text":"<p>The <code>upgrade</code> command automatically updates the k3s version in your cluster configuration file, but this step is crucial because:</p> <ol> <li>Updates Masters Configuration: Ensures that any new master nodes provisioned in the future will use the correct (new) k3s version instead of the previous version</li> <li>Updates Worker Node Templates: Updates the worker node pool configurations to ensure new worker nodes are created with the upgraded k3s version</li> <li>Synchronizes Cluster State: Ensures the actual cluster state matches the desired state defined in your configuration file</li> <li>Prevents Version Mismatch: Without this step, new nodes added to the cluster would be created with the old k3s version and would need to be upgraded again by the system upgrade controller, causing unnecessary delays and potential issues</li> </ol> <p>If you skip this step and add new nodes to the cluster later, they will first be created with the old k3s version and then need to be upgraded again, which is inefficient and can cause compatibility issues.</p>"},{"location":"Maintenance/#what-to-do-if-the-upgrade-doesnt-go-smoothly","title":"What to Do If the Upgrade Doesn\u2019t Go Smoothly","text":"<p>If the upgrade stalls or doesn\u2019t complete for all nodes:</p> <ol> <li>Clean up existing upgrade plans and jobs, then restart the upgrade controller:</li> </ol> <pre><code>kubectl -n system-upgrade delete job --all\nkubectl -n system-upgrade delete plan --all\n\nkubectl label node --all plan.upgrade.cattle.io/k3s-server- plan.upgrade.cattle.io/k3s-agent-\n\nkubectl -n system-upgrade rollout restart deployment system-upgrade-controller\nkubectl -n system-upgrade rollout status deployment system-upgrade-controller\n</code></pre> <p>You can also check the logs of the system upgrade controller\u2019s pod:</p> <pre><code>kubectl -n system-upgrade \\\n  logs -f $(kubectl -n system-upgrade get pod -l pod-template-hash -o jsonpath=\"{.items[0].metadata.name}\")\n</code></pre> <p>If the upgrade stalls after upgrading the masters but before upgrading the worker nodes, simply cleaning up resources might not be enough. In this case, run the following to mark the masters as upgraded and allow the upgrade to continue for the workers:</p> <pre><code>kubectl label node &lt;master1&gt; &lt;master2&gt; &lt;master2&gt; plan.upgrade.cattle.io/k3s-server=upgraded\n</code></pre> <p>Once all the nodes have been upgraded, remember to re-run the <code>hetzner-k3s create</code> command. This way, new nodes will be created with the new version right away. If you don\u2019t, they will first be created with the old version and then upgraded by the system upgrade controller.</p>"},{"location":"Maintenance/#upgrading-the-os-on-nodes","title":"Upgrading the OS on Nodes","text":"<ol> <li>Consider adding a temporary node during the process if your cluster doesn\u2019t have enough spare capacity.</li> <li>Drain one node.</li> <li>Update the OS and reboot the node.</li> <li>Uncordon the node.</li> <li>Repeat for the next node.</li> </ol> <p>To automate this process, you can install the Kubernetes Reboot Daemon (\"Kured\"). For Kured to work effectively, ensure the OS on your nodes has unattended upgrades enabled, at least for security updates. For example, if the image is Ubuntu, add this to the configuration file before running the <code>create</code> command:</p> <pre><code>additional_packages:\n- unattended-upgrades\n- update-notifier-common\nadditional_post_k3s_commands:\n- sudo systemctl enable unattended-upgrades\n- sudo systemctl start unattended-upgrades\n</code></pre> <p>Refer to the Kured documentation for additional configuration options, such as maintenance windows.</p>"},{"location":"Masters_in_different_locations/","title":"Masters in Different Locations","text":"<p>To ensure maximum availability, you can set up a regional cluster by placing each master in a different European location. The first master will be in Falkenstein, the second in Helsinki, and the third in Nuremberg (listed alphabetically). This setup is only possible in network zones with multiple locations, and currently, the only such zone is <code>eu-central</code>, which includes these three European locations. For other regions, only zonal clusters are supported. Regional clusters are limited to 3 masters because we only have these three locations available.</p> <p>To create a regional cluster, set the <code>instance_count</code> for the masters pool to 3 and specify the <code>locations</code> setting as <code>fsn1</code>, <code>hel1</code>, and <code>nbg1</code>.</p>"},{"location":"Masters_in_different_locations/#converting-a-single-master-or-zonal-cluster-to-a-regional-one","title":"Converting a Single Master or Zonal Cluster to a Regional One","text":"<p>If you already have a cluster with a single master or three masters in the same European location, converting it to a regional cluster is straightforward. Just follow these steps carefully and be patient. Note that this requires hetzner-k3s version 2.2.4 or higher.</p> <p>Before you begin, make sure to back up all your applications and data! This is crucial. While the migration process is relatively simple, there is always some level of risk involved.</p> <ul> <li>Set the <code>instance_count</code> for the masters pool to 3 if your cluster currently has only one master.</li> <li>Update the <code>locations</code> setting for the masters pool to include <code>fns1</code>, <code>hel1</code>, and <code>nbg1</code> like this:</li> </ul> <pre><code>locations:\n- fns1\n- hel1\n- nbg1\n</code></pre> <p>The locations are always processed in alphabetical order, regardless of how you list them in the <code>locations</code> property. This ensures consistency, especially when replacing a master due to node failure or other issues.</p> <ul> <li>If your cluster currently has a single master, run the <code>create</code> command with the updated configuration. This will create <code>master2</code> in Helsinki and <code>master3</code> in Nuremberg. Wait for the operation to complete and confirm that all three masters are in a ready state.</li> <li>If <code>master1</code> is not in Falkenstein (fns1):</li> <li>Drain <code>master1</code>.</li> <li>Delete <code>master1</code> using the command <code>kubectl delete node {cluster-name}-master1</code>.</li> <li>Remove the <code>master1</code> instance via the Hetzner Console or the <code>hcloud</code> utility.</li> <li>Run the <code>create</code> command again. This will recreate <code>master1</code> in Falkenstein.</li> <li>SSH into each master and run the following commands to ensure <code>master1</code> has joined the cluster correctly:</li> </ul> <pre><code>sudo apt-get update\nsudo apt-get install etcd-client\n\nexport ETCDCTL_API=3\nexport ETCDCTL_ENDPOINTS=https://[REDACTED].1:2379\nexport ETCDCTL_CACERT=/var/lib/rancher/k3s/server/tls/etcd/server-ca.crt\nexport ETCDCTL_CERT=/var/lib/rancher/k3s/server/tls/etcd/server-client.crt\nexport ETCDCTL_KEY=/var/lib/rancher/k3s/server/tls/etcd/server-client.key\n\netcdctl member list\n</code></pre> <p>The last command should display something like this if everything is working properly:</p> <pre><code>285ab4b980c2c8c, started, test-master2-d[REDACTED]af, https://[REDACTED]:2380, https://[REDACTED]:2379, false\naad3fac89b68bfb7, started, test-master1-5e550de0, https://[REDACTED]:2380, https://[REDACTED]:2379, false\nc[REDACTED]e25aef34e8, started, test-master3-0ed051a3, https://[REDACTED]:2380, https://[REDACTED]:2379, false\n</code></pre> <ul> <li>If <code>master2</code> is not in Helsinki, follow the same steps as with <code>master1</code> but for <code>master2</code>. This will recreate <code>master2</code> in Helsinki.</li> <li>If <code>master3</code> is not in Nuremberg, repeat the process for <code>master3</code>. This will recreate <code>master3</code> in Nuremberg.</li> </ul> <p>That\u2019s it! You now have a regional cluster, which ensures continued operation even if one of the Hetzner locations experiences a temporary failure. I also recommend enabling the <code>create_load_balancer_for_the_kubernetes_api</code> setting to <code>true</code> if you don\u2019t already have a load balancer for the Kubernetes API.</p>"},{"location":"Masters_in_different_locations/#performance-considerations","title":"Performance Considerations","text":"<p>This feature has been frequently requested, but I delayed implementing it until I could thoroughly test the configuration. I was concerned about latency issues, as etcd is sensitive to delays, and I wanted to ensure that the latency between the German locations and Helsinki wouldn\u2019t cause problems.</p> <p>It turns out that the default heartbeat interval for etcd is 100ms, and the latency between Helsinki and Falkenstein/Nuremberg is only 25-27ms. This means the total round-trip time (RTT) for the Raft consensus is around 60-70ms, which is well within etcd\u2019s acceptable limits. After running benchmarks, everything works smoothly! So, there\u2019s no need to adjust the etcd configuration for this setup.</p>"},{"location":"Private_clusters_with_public_network_interface_disabled/","title":"Private clusters with public network interface disabled","text":"<p>By default, network access to nodes in a cluster created with hetzner-k3s is limited to the networks listed in the configuration file. Some users might want to completely turn off the public interface on their nodes instead.</p> <p>This page offers a reference configuration to help you disable the public interface. Keep in mind that some steps might vary depending on the operating system you choose for your nodes. The example configuration has been tested successfully with Debian 12, as it's a bit simpler to work with compared to other OSes.</p> <p>Please note that this configuration is designed for new clusters only. I haven't tested if it works to convert an existing cluster to one with the public interface disabled.</p> <p>When setting up a cluster with disabled public network interfaces, remember you'll need a NAT gateway to access the cluster from outside Hetzner Cloud. Without it, your nodes won't be able to connect to the internet, and hetzner-k3s won\u2019t be able to install k3s on those nodes.</p> <p>Another important thing to consider is that with the public network interface disabled on all nodes, you can't run hetzner-k3s from a computer outside the cluster's private network. So, you'll need to run hetzner-k3s from a cloud instance within the private network. You could use the same instance you're using as your NAT gateway for this purpose too.</p>"},{"location":"Private_clusters_with_public_network_interface_disabled/#prerequisite-nat-gateway","title":"Prerequisite: NAT Gateway","text":"<p>First off, you need to set up a NAT gateway for your Hetzner Cloud network. Follow the instructions on this page.</p> <p>The guide uses Debian as an example, so make sure to review the page and adjust any settings according to the operating system you choose for your cluster nodes and the NAT gateway instance.</p> <p>The TL;DR is this:</p> <ul> <li> <p> First, create a private network in the Hetzner project where your cluster will reside. Choose any subnet you like, but for reference purposes, let's assume it\u2019s 10.0.0.0/16.</p> </li> <li> <p> Next, set up a cloud instance to act as your NAT gateway. Ensure it has a public IP address and connects to the private network you just created.</p> </li> <li> <p> Then, add a route on your private network for <code>0.0.0.0/0</code>, directing it to the IP address of your NAT gateway instance, which you can select from a dropdown menu.</p> </li> <li> <p> Finally, on the NAT gateway instance itself, tweak <code>/etc/network/interfaces</code>. Add these lines or adjust existing ones for your private network interface:</p> </li> </ul> <pre><code>auto enp7s0\niface enp7s0 inet dhcp\n    post-up echo 1 &gt; /proc/sys/net/ipv4/ip_forward\n    post-up iptables -t nat -A POSTROUTING -s '10.0.0.0/16' -o enp7s0 -j MASQUERADE\n</code></pre> <p>Replace <code>10.0.0.0/16</code> with your actual subnet if it's different. Also, make sure to use the correct name for your private network interface if <code>enp7s0</code> isn't right\u2014find this with the <code>ifconfig</code> command.</p> <ul> <li> Lastly, restart your NAT gateway instance to apply these changes.</li> </ul>"},{"location":"Private_clusters_with_public_network_interface_disabled/#cluster-configuration","title":"Cluster configuration","text":"<ul> <li> Edit the configuration file for your cluster and set both <code>ipv4</code> and <code>ipv6</code> to <code>false</code>, plus reference the existing private network you have already created:</li> </ul> <pre><code>  public_network:\n    ipv4: false\n    ipv6: false\n  private_network:\n    enabled : true\n    subnet: 10.0.0.0/16\n    existing_network_name: \"&lt;name of your private network&gt;\"\n</code></pre> <p>Also configure the allowed networks:</p> <pre><code>  allowed_networks:\n    ssh:\n      - 0.0.0.0/0\n    api:\n      - 0.0.0.0/0\n</code></pre> <ul> <li> <p> Since you're setting up a private cluster, it makes sense to turn off the load balancer for the Kubernetes API. You can do this by setting <code>create_load_balancer_for_the_kubernetes_api</code> to <code>false</code>.</p> </li> <li> <p> Also, if you want to use an OS image other than the default (<code>ubuntu-24.04</code>), you can configure it accordingly. For example, if you prefer Debian 12, you can set it up like this:</p> </li> </ul> <pre><code>image: debian-12\nautoscaling_image: debian-12\n</code></pre> <ul> <li> Next, you need to set up network configuration commands. These steps will ensure that the nodes in your clusters use the NAT gateway to access the Internet. You can use either <code>additional_pre_k3s_commands</code> (before k3s installation) or <code>additional_post_k3s_commands</code> (after k3s installation) depending on your needs.</li> </ul> <p>For <code>ubuntu-24.04</code>:</p> <pre><code>additional_pre_k3s_commands:\n- printf \"[Match]\\nName=enp7s0\\n\\n[Network]\\nDHCP=yes\\nGateway=10.0.0.1\\n\" &gt; /etc/systemd/network/10-enp7s0.network\n- printf \"[Resolve]\\nDNS=185.12.64.2 185.12.64.1\" &gt; /etc/systemd/resolved.conf\n- systemctl restart systemd-networkd\n- systemctl restart systemd-resolved\n</code></pre> <p>For <code>debian-12</code>:</p> <pre><code>additional_pre_k3s_commands:\n- apt update\n- apt upgrade -y\n- apt install ifupdown resolvconf -y\n- apt autoremove -y hc-utils\n- apt purge -y hc-utils\n- echo \"auto enp7s0\" &gt; /etc/network/interfaces.d/60-private\n- echo \"iface enp7s0 inet dhcp\" &gt;&gt; /etc/network/interfaces.d/60-private\n- echo \"    post-up ip route add default via 10.0.0.1\"  &gt;&gt; /etc/network/interfaces.d/60-private\n- echo \"[Resolve]\" &gt; /etc/systemd/resolved.conf\n- echo \"DNS=1.1.1.1 1.0.0.1\" &gt;&gt; /etc/systemd/resolved.conf\n- ifdown enp7s0\n- ifup enp7s0\n- systemctl start resolvconf\n- systemctl enable resolvconf\n- echo \"nameserver 1.1.1.1\" &gt;&gt; /etc/resolvconf/resolv.conf.d/head\n- echo \"nameserver 1.0.0.1\" &gt;&gt; /etc/resolvconf/resolv.conf.d/head\n- resolvconf --enable-updates\n- resolvconf -u\n</code></pre> <p>Note about Ubuntu vs Debian: The Debian configuration requires installing <code>ifupdown</code> and related packages, which cannot be done on Ubuntu without internet access. Ubuntu 24.04 uses systemd-networkd by default, making the configuration simpler and more suitable for private cluster setups where internet access is only available after the NAT gateway configuration is complete.</p> <p>Replace <code>enp7s0</code> with your network interface name, and <code>10.0.0.1</code> with the correct gateway IP address for your subnet. Note that this is not the IP address of the NAT gateway instance; it's simply the first IP in the range.</p> <p>One important thing to remember: these simple commands work great if you're using the same type of instances, like all AMD instances, for both your master and worker node pools. We're referencing a specific private network interface name here.</p> <p>If you plan on using different types of instances in your cluster, you'll need to tweak these commands to use a more flexible method for identifying the correct private interface on each node.</p>"},{"location":"Private_clusters_with_public_network_interface_disabled/#creating-the-cluster","title":"Creating the cluster","text":"<p>Run the <code>create</code> command with hetzner-k3s as usual, but use this updated configuration from an instance connected to the same private network. For example, you can use the NAT gateway instance if you don't want to create another one.</p> <p>The nodes will be able to access the Internet through the NAT gateway. Therefore, hetzner-k3s should complete creating the cluster successfully.</p>"},{"location":"Recommendations/","title":"Recommendations","text":"<p>This page provides best practices and recommendations for different cluster sizes and use cases with hetzner-k3s.</p>"},{"location":"Recommendations/#small-to-medium-clusters-1-50-nodes","title":"Small to Medium Clusters (1-50 nodes)","text":"<p>The default configuration works well for small to medium-sized clusters, providing a simple, reliable setup with minimal configuration required.</p>"},{"location":"Recommendations/#key-considerations","title":"Key Considerations","text":"<ul> <li>Private Network: Enabled by default for better security</li> <li>CNI: Flannel for simplicity or Cilium for advanced features</li> <li>Storage: <code>hcloud-volumes</code> for persistence</li> <li>Load Balancers: Hetzner Load Balancers for production workloads</li> <li>High Availability: 3 master nodes for production clusters</li> </ul>"},{"location":"Recommendations/#recommended-configuration","title":"Recommended Configuration","text":"<pre><code>hetzner_token: &lt;your token&gt;\ncluster_name: my-cluster\nkubeconfig_path: \"./kubeconfig\"\nk3s_version: v1.32.0+k3s1\n\nnetworking:\n  ssh:\n    port: 22\n    use_agent: false\n    public_key_path: \"~/.ssh/id_ed25519.pub\"\n    private_key_path: \"~/.ssh/id_ed25519\"\n  allowed_networks:\n    ssh:\n      - 0.0.0.0/0\n    api:\n      - 10.0.0.0/16  # Restrict to private network\n  public_network:\n    ipv4: true\n    ipv6: true\n  private_network:\n    enabled: true\n    subnet: 10.0.0.0/16\n  cni:\n    enabled: true\n    encryption: false\n    mode: flannel\n\nmasters_pool:\n  instance_type: cpx22\n  instance_count: 3  # For HA\n  locations:\n    - nbg1\n\nworker_node_pools:\n- name: workers\n  instance_type: cpx32\n  instance_count: 3\n  location: nbg1\n  autoscaling:\n    enabled: true\n    min_instances: 1\n    max_instances: 5\n\nprotect_against_deletion: true\ncreate_load_balancer_for_the_kubernetes_api: true\n</code></pre>"},{"location":"Recommendations/#large-clusters-50-nodes","title":"Large Clusters (50+ nodes)","text":"<p>For larger clusters, the default setup has some limitations that need to be addressed.</p>"},{"location":"Recommendations/#limitations-of-default-setup","title":"Limitations of Default Setup","text":"<p>Hetzner's private networks, used in hetzner-k3s' default configuration, only support up to 100 nodes. If your cluster is going to grow beyond that, you need to disable the private network in your configuration, to use the public network instead (no worries, all traffic between nodes is automatically encrypted with Wireguard).</p>"},{"location":"Recommendations/#large-cluster-architecture-since-v228","title":"Large Cluster Architecture (Since v2.2.8)","text":"<p>Support for large clusters has significantly improved since version 2.2.8. The main changes include:</p> <ol> <li>Custom Firewall: Instead of using Hetzner's firewall (which is slow to update), a custom firewall solution was implemented</li> <li>IP Query Server: A simple container that checks the Hetzner API every 30 seconds to get the list of all node IPs</li> <li>Automatic Updates: Firewall rules are automatically updated without manual intervention</li> </ol>"},{"location":"Recommendations/#setting-up-large-clusters","title":"Setting Up Large Clusters","text":""},{"location":"Recommendations/#step-1-set-up-ip-query-server","title":"Step 1: Set Up IP Query Server","text":"<p>The IP query server runs as a simple container. You can easily set it up on any Docker-enabled server using the <code>docker-compose.yml</code> file in the <code>ip-query-server</code> folder of this repository.</p> <pre><code># docker-compose.yml\nversion: '3.8'\nservices:\n  ip-query-server:\n    build: ./ip-query-server\n    ports:\n      - \"8080:80\"\n    environment:\n      - HETZNER_TOKEN=your_token_here\n  caddy:\n    image: caddy:2\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - ./Caddyfile:/etc/caddy/Caddyfile\n    depends_on:\n      - ip-query-server\n</code></pre> <p>Replace <code>example.com</code> in the Caddyfile with your actual domain name and <code>mail@example.com</code> with your email address for Let's Encrypt certificates.</p>"},{"location":"Recommendations/#step-2-update-cluster-configuration","title":"Step 2: Update Cluster Configuration","text":"<pre><code>hetzner_token: &lt;your token&gt;\ncluster_name: large-cluster\nkubeconfig_path: \"./kubeconfig\"\nk3s_version: v1.32.0+k3s1\n\nnetworking:\n  ssh:\n    port: 22\n    use_agent: true  # Recommended for large clusters\n    public_key_path: \"~/.ssh/id_ed25519.pub\"\n    private_key_path: \"~/.ssh/id_ed25519\"\n  allowed_networks:\n    ssh:\n      - 0.0.0.0/0  # Required for public network access\n    api:\n      - 0.0.0.0/0  # Required when private network is disabled\n  public_network:\n    ipv4: true\n    ipv6: true\n    # Use custom IP query server for large clusters\n    hetzner_ips_query_server_url: https://ip-query.example.com\n    use_local_firewall: true  # Enable custom firewall\n  private_network:\n    enabled: false  # Disable private network for &gt;100 nodes\n  cni:\n    enabled: true\n    encryption: true  # Enable encryption for public network\n    mode: cilium  # Better for large scale deployments\n\n# Larger cluster CIDR ranges\ncluster_cidr: 10.244.0.0/15  # Larger range for more pods\nservice_cidr: 10.96.0.0/16   # Larger range for more services\ncluster_dns: 10.96.0.10\n\ndatastore:\n  mode: etcd  # or external for very large clusters\n  # external_datastore_endpoint: postgres://...\n\nmasters_pool:\n  instance_type: cpx32\n  instance_count: 3\n  locations:\n    - nbg1\n    - hel1\n    - fsn1\n\nworker_node_pools:\n- name: compute\n  instance_type: cpx42\n  location: nbg1\n  autoscaling:\n    enabled: true\n    min_instances: 5\n    max_instances: 50\n- name: storage\n  instance_type: cpx52\n  location: hel1\n  autoscaling:\n    enabled: true\n    min_instances: 3\n    max_instances: 20\n\naddons:\n  embedded_registry_mirror:\n    enabled: true  # Recommended for large clusters\n\nprotect_against_deletion: true\ncreate_load_balancer_for_the_kubernetes_api: true\nk3s_upgrade_concurrency: 2  # Can upgrade more nodes simultaneously\n</code></pre>"},{"location":"Recommendations/#additional-large-cluster-considerations","title":"Additional Large Cluster Considerations","text":""},{"location":"Recommendations/#network-configuration","title":"Network Configuration","text":"<ul> <li>CIDR Sizing: Use larger cluster and service CIDR ranges to accommodate more pods and services</li> <li>Encryption: Enable CNI encryption when using public networks</li> <li>Firewall: The custom firewall automatically manages allowed IPs without opening ports to the public</li> </ul>"},{"location":"Recommendations/#high-availability-setup","title":"High Availability Setup","text":"<p>For production large clusters, consider:</p> <ol> <li>Multiple IP Query Servers: Set up 2-3 instances behind a load balancer for better availability</li> <li>External Datastore: Use PostgreSQL instead of etcd for better scalability</li> <li>Distributed Master Nodes: Place masters in different locations</li> <li>Multiple Node Pools: Different instance types for different workloads</li> </ol>"},{"location":"Recommendations/#cluster-sizing-guidelines","title":"Cluster Sizing Guidelines","text":""},{"location":"Recommendations/#developmenttiny-clusters-5-nodes","title":"Development/Tiny Clusters (&lt; 5 nodes)","text":"<pre><code>masters_pool:\n  instance_type: cpx11\n  instance_count: 1  # Single master for testing\nworker_node_pools:\n- name: workers\n  instance_type: cpx11\n  instance_count: 1\n</code></pre>"},{"location":"Recommendations/#small-production-clusters-5-20-nodes","title":"Small Production Clusters (5-20 nodes)","text":"<pre><code>masters_pool:\n  instance_type: cpx22\n  instance_count: 3  # HA masters\n  locations:\n    - fsn1\n    - hel1\n    - nbg1\nworker_node_pools:\n- name: workers\n  instance_type: cpx32\n  instance_count: 3\n  autoscaling:\n    enabled: true\n    min_instances: 1\n    max_instances: 5\n</code></pre>"},{"location":"Recommendations/#medium-production-clusters-20-50-nodes","title":"Medium Production Clusters (20-50 nodes)","text":"<pre><code>masters_pool:\n  instance_type: cpx32\n  instance_count: 3\n  locations:\n    - fsn1\n    - hel1\n    - nbg1\nworker_node_pools:\n- name: web\n  instance_type: cpx32\n  location: nbg1\n  autoscaling:\n    enabled: true\n    min_instances: 3\n    max_instances: 10\n- name: backend\n  instance_type: cpx42\n  location: hel1\n  autoscaling:\n    enabled: true\n    min_instances: 2\n    max_instances: 8\n</code></pre>"},{"location":"Recommendations/#large-production-clusters-50-200-nodes","title":"Large Production Clusters (50-200+ nodes)","text":"<p>Use the large cluster configuration shown above with: - Multiple node pools for different workloads - Custom firewall and IP query server - Larger instance types for masters - External datastore if needed</p>"},{"location":"Recommendations/#performance-optimization","title":"Performance Optimization","text":""},{"location":"Recommendations/#embedded-registry-mirror","title":"Embedded Registry Mirror","text":"<p>In v2.0.0, there's a new option to enable the <code>embedded registry mirror</code> in k3s. You can find more details here. This feature uses Spegel to enable peer-to-peer distribution of container images across cluster nodes.</p> <p>Benefits: - Faster pod startup times - Reduced external registry calls - Better reliability when external registries are inaccessible - Cost savings on egress bandwidth</p> <p>Configuration: <pre><code>embedded_registry_mirror:\n  enabled: true\n</code></pre></p> <p>Note: Ensure your k3s version supports this feature before enabling.</p>"},{"location":"Recommendations/#storage-selection","title":"Storage Selection","text":""},{"location":"Recommendations/#use-hcloud-volumes-for","title":"Use <code>hcloud-volumes</code> for:","text":"<ul> <li>Production databases where the app does not take care of replication already</li> <li>Persistent application data</li> <li>Content that must survive pod restarts</li> <li>Applications requiring high availability</li> </ul>"},{"location":"Recommendations/#use-local-path-for","title":"Use <code>local-path</code> for:","text":"<ul> <li>High-performance caching (Redis, Memcached)</li> <li>High-performance databases (Postgres, MySQL) where the app takes care of replication already</li> <li>Temporary file storage</li> <li>Applications that can tolerate data loss</li> <li>Maximum IOPS performance</li> </ul>"},{"location":"Recommendations/#cni-selection","title":"CNI Selection","text":""},{"location":"Recommendations/#flannel","title":"Flannel","text":"<ul> <li>Pros: Simple, lightweight, good for small clusters</li> <li>Cons: Limited features, doesn't scale well to very large clusters</li> <li>Best for: Small to medium clusters, simplicity</li> </ul>"},{"location":"Recommendations/#cilium","title":"Cilium","text":"<ul> <li>Pros: Advanced features, better performance scales well</li> <li>Cons: More complex setup, higher resource usage</li> <li>Best for: Medium to large clusters, advanced networking needs</li> </ul>"},{"location":"Recommendations/#security-recommendations","title":"Security Recommendations","text":""},{"location":"Recommendations/#network-security","title":"Network Security","text":"<ol> <li>Restrict SSH and API Access: Use CIDR restrictions in <code>allowed_networks.api</code> and <code>allowed_networks.ssh</code></li> <li>Use Private Networks: When possible, use private networks for cluster communication</li> <li>Monitor Network Traffic: Implement network policies and monitoring</li> </ol>"},{"location":"Recommendations/#ssh-security","title":"SSH Security","text":"<ol> <li>Use SSH Keys: hetzner-k3s configures nodes with SSH keys by default</li> <li>SSH Agent: Enable <code>use_agent: true</code> for passphrase-protected keys</li> <li>Key Rotation: Regularly rotate SSH keys if needed</li> <li>Access Logs: Monitor SSH access logs</li> </ol>"},{"location":"Recommendations/#cluster-security","title":"Cluster Security","text":"<ol> <li>RBAC: Implement proper role-based access control</li> <li>Network Policies: Use Kubernetes network policies</li> <li>Pod Security: Implement pod security standards</li> <li>Regular Updates: Keep k3s and components updated</li> </ol>"},{"location":"Recommendations/#cost-optimization","title":"Cost Optimization","text":""},{"location":"Recommendations/#instance-selection","title":"Instance Selection","text":"<ul> <li>Right-size Instances: Start smaller and scale up as needed</li> <li>Use Autoscaling: Only pay for what you use</li> </ul>"},{"location":"Recommendations/#storage-optimization","title":"Storage Optimization","text":"<ul> <li>Clean Up Volumes: Regularly delete unused volumes</li> <li>Use Local Storage: For temporary data where appropriate</li> <li>Monitor Usage: Set up monitoring to identify unused storage</li> </ul>"},{"location":"Recommendations/#network-optimization","title":"Network Optimization","text":"<ul> <li>Use Private Networks: Reduce egress costs</li> <li>Optimize Images: Use smaller container images</li> <li>Registry Mirror: Reduce registry egress costs</li> </ul>"},{"location":"Recommendations/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"Recommendations/#essential-monitoring","title":"Essential Monitoring","text":"<ol> <li>Node Resources: CPU, memory, disk usage</li> <li>Cluster Health: Node readiness, pod status</li> <li>Network Traffic: Bandwidth usage, connection counts</li> <li>Storage Performance: I/O operations, latency</li> </ol>"},{"location":"Recommendations/#recommended-tools","title":"Recommended Tools","text":"<ul> <li>Prometheus + Grafana: For metrics and dashboards</li> <li>Loki: For log aggregation</li> <li>Alertmanager: For alerting</li> <li>Node Exporter: For node metrics</li> </ul>"},{"location":"Resizing_root_partition_with_post_create_commands/","title":"Resizing root partition with additional post k3s commands","text":"<p>When deploying storage solutions like Rook Ceph, you may need to resize the root partition to free up disk space for use by the storage system. This guide shows you how to use the <code>additional_post_k3s_commands</code> setting to automate this process.</p>"},{"location":"Resizing_root_partition_with_post_create_commands/#overview","title":"Overview","text":"<p>By default, Hetzner Cloud instances automatically expand the root partition to use the entire available disk space. To manually manage partitions (e.g., for Rook Ceph storage), you need to disable this automatic behavior and then use custom commands to resize partitions according to your needs.</p> <p>The following commands will:</p> <ol> <li>Expand the GPT partition table to use the entire disk</li> <li>Resize the root partition to use 50% of the disk</li> <li>Create a new partition using the remaining space</li> <li>Expand the filesystem to use the entire root partition</li> </ol>"},{"location":"Resizing_root_partition_with_post_create_commands/#prerequisites","title":"Prerequisites","text":"<p>Disable automatic root partition growth by setting <code>grow_root_partition_automatically</code> to <code>false</code>. You can set this globally for all nodes or override it per node pool.</p>"},{"location":"Resizing_root_partition_with_post_create_commands/#global-setting","title":"Global Setting","text":"<p>Apply to all nodes in the cluster:</p> <pre><code>grow_root_partition_automatically: false\n</code></pre>"},{"location":"Resizing_root_partition_with_post_create_commands/#per-node-pool-override","title":"Per Node Pool Override","text":"<p>Configure different partitioning strategies per node pool:</p> <pre><code>worker_node_pools:\n- name: storage-workers\n  instance_type: cpx32\n  location: fsn1\n  grow_root_partition_automatically: false  # Disable for storage nodes\n  additional_post_k3s_commands:\n  - [ sgdisk, -e, /dev/sda ]\n  - [ partprobe ]\n  - [ parted, -s, /dev/sda, mkpart, primary, ext4, \"50%\", \"100%\" ]\n  - [ growpart, /dev/sda, \"1\" ]\n  - [ resize2fs, /dev/sda1 ]\n\n- name: regular-workers\n  instance_type: cpx22\n  location: hel1\n  # Inherits global setting (or true if global is not set)\n  # grow_root_partition_automatically: true  # automatic growth\n</code></pre> <p>How it works: - Global setting defaults to <code>true</code> (automatic growth) - Per-pool setting overrides global setting - If not specified per pool, inherits global setting - When <code>false</code>, creates <code>/etc/growroot-disabled</code> to prevent automatic growth</p>"},{"location":"Resizing_root_partition_with_post_create_commands/#partition-commands","title":"Partition Commands","text":"<p>Add these <code>additional_post_k3s_commands</code> to disable automatic growth and manually resize partitions:</p> <pre><code>additional_post_k3s_commands:\n- [ sgdisk, -e, /dev/sda ]\n- [ partprobe ]\n- [ parted, -s, /dev/sda, mkpart, primary, ext4, \"50%\", \"100%\" ]\n- [ growpart, /dev/sda, \"1\" ]\n- [ resize2fs, /dev/sda1 ]\n</code></pre>"},{"location":"Resizing_root_partition_with_post_create_commands/#command-breakdown","title":"Command Breakdown","text":"<ol> <li><code>[ sgdisk, -e, /dev/sda ]</code></li> <li> <p>Expands the GPT partition table to use the entire disk space</p> </li> <li> <p><code>[ partprobe ]</code></p> </li> <li> <p>Notifies the kernel of partition table changes</p> </li> <li> <p><code>[ parted, -s, /dev/sda, mkpart, primary, ext4, \"50%\", \"100%\" ]</code></p> </li> <li>Creates a new partition using the remaining 50% of disk space</li> <li> <p>Available for Rook Ceph or other storage solutions</p> </li> <li> <p><code>[ growpart, /dev/sda, \"1\" ]</code></p> </li> <li> <p>Resizes root partition (partition 1) to use 50% of the disk</p> </li> <li> <p><code>[ resize2fs, /dev/sda1 ]</code></p> </li> <li>Expands the ext4 filesystem to use the entire root partition</li> </ol>"},{"location":"Resizing_root_partition_with_post_create_commands/#result","title":"Result","text":"<p>After running these commands:</p> <ul> <li>Root partition (<code>/dev/sda1</code>) uses 50% of disk space</li> <li>New partition available for storage solutions like Rook Ceph</li> <li>Filesystem expanded to use entire root partition</li> </ul>"},{"location":"Resizing_root_partition_with_post_create_commands/#important-notes","title":"Important Notes","text":"<ul> <li>Device Names: Commands assume root disk is <code>/dev/sda</code> and root partition is <code>/dev/sda1</code>. Adjust if needed.</li> <li>Test First: Test on a non-critical node before production use.</li> <li>Backup Data: These commands are destructive. Backup important data before applying.</li> <li>Root Privileges: Commands run as root, which is standard for <code>additional_post_k3s_commands</code>.</li> </ul>"},{"location":"Resizing_root_partition_with_post_create_commands/#example-configuration","title":"Example Configuration","text":"<p>Complete cluster configuration with disk resizing for storage nodes:</p> <pre><code>grow_root_partition_automatically: true  # Default for most nodes\n\nmasters_pool:\n  instance_type: cpx22\n  instance_count: 3\n  locations: [fsn1, hel1, nbg1]\n  # Inherits global: true (automatic growth)\n\nworker_node_pools:\n- name: storage-workers\n  instance_type: cpx32\n  instance_count: 4\n  location: fsn1\n  grow_root_partition_automatically: false  # Override: manual partitioning\n  additional_post_k3s_commands:\n  - [ sgdisk, -e, /dev/sda ]\n  - [ partprobe ]\n  - [ parted, -s, /dev/sda, mkpart, primary, ext4, \"50%\", \"100%\" ]\n  - [ growpart, /dev/sda, \"1\" ]\n  - [ resize2fs, /dev/sda1 ]\n\n- name: regular-workers\n  instance_type: cpx22\n  instance_count: 2\n  location: hel1\n  # Inherits global: true (automatic growth)\n\n# Other cluster settings...\n</code></pre> <p>This setup gives you flexibility: regular nodes use automatic growth while storage nodes use custom partitioning optimized for Rook Ceph or similar storage solutions.</p>"},{"location":"Run_command/","title":"The 'run' Command","text":"<p>The <code>hetzner-k3s run</code> command allows you to execute a single command or an entire script on either all nodes in your cluster or on a specific instance. This is particularly useful for maintenance tasks, configuration updates, and automated operations across your cluster.</p>"},{"location":"Run_command/#command-overview","title":"Command Overview","text":"<pre><code>hetzner-k3s run --config &lt;config-file&gt; [options]\n</code></pre>"},{"location":"Run_command/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>--config</code>, <code>-c</code>: The path to your cluster configuration YAML file</li> </ul>"},{"location":"Run_command/#execution-modes","title":"Execution Modes","text":""},{"location":"Run_command/#1-running-commands","title":"1. Running Commands","text":"<p>Execute a single command on all cluster nodes:</p> <pre><code>hetzner-k3s run --config cluster.yaml --command \"sudo apt update &amp;&amp; sudo apt upgrade -y\"\n</code></pre> <p>Execute a single command on a specific instance:</p> <pre><code>hetzner-k3s run --config cluster.yaml --command \"hostname\" --instance \"worker-node-1\"\n</code></pre>"},{"location":"Run_command/#2-running-scripts","title":"2. Running Scripts","text":"<p>Execute a script file on all cluster nodes:</p> <pre><code>hetzner-k3s run --config cluster.yaml --script fix-ssh.sh\n</code></pre> <p>Execute a script file on a specific instance:</p> <pre><code>hetzner-k3s run --config cluster.yaml --script fix-ssh.sh --instance \"master-node-1\"\n</code></pre>"},{"location":"Run_command/#option-parameters","title":"Option Parameters","text":"<ul> <li><code>--command</code>: The shell command to execute</li> <li><code>--script</code>: The path to a script file to execute</li> <li><code>--instance</code>: The name of a specific instance to run the command/script on (if not specified, runs on all instances)</li> </ul> <p>Note: You must specify exactly one of either <code>--command</code> or <code>--script</code>.</p>"},{"location":"Run_command/#examples","title":"Examples","text":""},{"location":"Run_command/#example-1-check-system-information-on-all-nodes","title":"Example 1: Check system information on all nodes","text":"<pre><code>hetzner-k3s run --config my-cluster.yaml --command \"uname -a &amp;&amp; df -h\"\n</code></pre>"},{"location":"Run_command/#example-2-update-packages-on-a-specific-worker-node","title":"Example 2: Update packages on a specific worker node","text":"<pre><code>hetzner-k3s run --config my-cluster.yaml --command \"sudo apt update &amp;&amp; sudo apt list --upgradable\" --instance worker-1\n</code></pre>"},{"location":"Run_command/#example-3-run-the-ssh-fix-script-on-all-nodes","title":"Example 3: Run the SSH fix script on all nodes","text":"<pre><code>hetzner-k3s run --config my-cluster.yaml --script fix-ssh.sh\n</code></pre>"},{"location":"Run_command/#example-4-run-a-custom-maintenance-script-on-master-node-only","title":"Example 4: Run a custom maintenance script on master node only","text":"<pre><code>hetzner-k3s run --config my-cluster.yaml --script maintenance.sh --instance master-1\n</code></pre>"},{"location":"Run_command/#how-it-works","title":"How It Works","text":""},{"location":"Run_command/#command-execution","title":"Command Execution","text":"<p>When using <code>--command</code>, hetzner-k3s: 1. Connects to each instance via SSH 2. Executes the specified command directly 3. Captures and displays the output 4. Returns the command completion status</p>"},{"location":"Run_command/#script-execution","title":"Script Execution","text":"<p>When using <code>--script</code>, hetzner-k3s: 1. Validates the script file exists and is readable 2. Uploads the script to <code>/tmp/&lt;script-name&gt;</code> on each instance 3. Makes the script executable 4. Executes the script 5. Captures and displays the output 6. Automatically cleans up by removing the uploaded script file</p>"},{"location":"Run_command/#parallel-execution","title":"Parallel Execution","text":"<p>The <code>run</code> command executes operations in parallel across all instances, significantly reducing the time required for cluster-wide operations. Each instance's output is displayed separately for clarity.</p>"},{"location":"Run_command/#user-confirmation","title":"User Confirmation","text":"<p>Before execution, the command displays: - A summary of instances that will be affected - The command or script to be executed - A confirmation prompt requiring you to type \"continue\" to proceed</p>"},{"location":"Run_command/#error-handling","title":"Error Handling","text":"<p>The command handles various error scenarios:</p> <ul> <li>SSH Connection Issues: If SSH connection fails, the error is displayed and execution continues on other instances</li> <li>Script File Not Found: If the specified script file doesn't exist, the command exits with an error</li> <li>Permission Issues: If the script file is not readable, the command exits with an error</li> <li>Instance Not Found: If a specific instance name doesn't exist in the cluster, the command exits with an error</li> </ul>"},{"location":"Run_command/#output-format","title":"Output Format","text":"<p>Output is organized by instance, making it easy to identify which node produced which output:</p> <pre><code>Found 3 instances in the cluster\nCommand to execute: hostname\n\nNodes that will be affected:\n  - master-1 (192.168.1.100)\n  - worker-1 (192.168.1.101)\n  - worker-2 (192.168.1.102)\n\nType 'continue' to execute this command on all nodes: continue\n\n=== Instance: master-1 (192.168.1.100) ===\nmaster-1\nCommand completed successfully\n\n=== Instance: worker-1 (192.168.1.101) ===\nworker-1\nCommand completed successfully\n\n=== Instance: worker-2 (192.168.1.102) ===\nworker-2\nCommand completed successfully\n</code></pre>"},{"location":"Run_command/#security-considerations","title":"Security Considerations","text":"<ul> <li>Commands and scripts are executed with the permissions of the SSH user</li> <li>Use <code>sudo</code> within commands/scripts when root privileges are required</li> <li>Scripts are uploaded to <code>/tmp/</code> and executed from there, then automatically cleaned up</li> <li>Ensure your script files have appropriate permissions and are secure</li> </ul>"},{"location":"Run_command/#use-cases","title":"Use Cases","text":""},{"location":"Run_command/#maintenance-operations","title":"Maintenance Operations","text":"<ul> <li>System updates: <code>--command \"sudo apt update &amp;&amp; sudo apt upgrade -y\"</code></li> <li>Log cleanup: <code>--command \"sudo journalctl --vacuum-time=7d\"</code></li> <li>Service restarts: <code>--command \"sudo systemctl restart docker\"</code></li> </ul>"},{"location":"Run_command/#configuration-management","title":"Configuration Management","text":"<ul> <li>Apply configuration changes across all nodes</li> <li>Deploy configuration files using scripts</li> <li>Update system settings</li> </ul>"},{"location":"Run_command/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Check system status: <code>--command \"systemctl status\"</code></li> <li>Examine logs: <code>--command \"journalctl -u k3s-agent -n 50\"</code></li> <li>Verify network connectivity: <code>--command \"ping -c 3 google.com\"</code></li> </ul>"},{"location":"Run_command/#security-updates","title":"Security Updates","text":"<ul> <li>Apply security patches cluster-wide</li> <li>Update SSH configurations (like the fix-ssh.sh script)</li> <li>Modify firewall rules</li> </ul>"},{"location":"Run_command/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ol> <li>Test on a single instance first: Use <code>--instance</code> to test commands/scripts on one node before applying to all nodes</li> <li>Use idempotent operations: Design commands/scripts to be safe to run multiple times</li> <li>Capture output: For long-running operations, consider redirecting output to files</li> <li>Handle errors gracefully: Include error handling in your scripts when appropriate</li> <li>Use absolute paths: In scripts, prefer absolute paths to avoid path-related issues</li> </ol>"},{"location":"Run_command/#integration-with-cluster-operations","title":"Integration with Cluster Operations","text":"<p>The <code>run</code> command is particularly powerful when combined with other hetzner-k3s operations:</p> <ul> <li>Use after cluster creation to apply initial configurations</li> <li>Run pre-upgrade checks before upgrading cluster components</li> <li>Execute post-upgrade verification commands</li> <li>Apply security patches across the entire cluster efficiently</li> </ul>"},{"location":"Setting_up_a_cluster/","title":"Complete Tutorial","text":"<p>By TitanFighter</p>"},{"location":"Setting_up_a_cluster/#setting-up-a-cluster","title":"Setting Up a Cluster","text":"<p>This guide will walk you through creating a fully functional Kubernetes cluster on Hetzner Cloud using hetzner-k3s, complete with ingress controller and a sample application.</p>"},{"location":"Setting_up_a_cluster/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ol> <li>Hetzner Cloud Account with project and API token</li> <li>SSH Key Pair for accessing cluster nodes</li> <li>kubectl installed on your local machine (installation guide)</li> <li>Helm installed on your local machine (installation guide)</li> <li>hetzner-k3s installed on your local machine (installation guide)</li> </ol>"},{"location":"Setting_up_a_cluster/#instructions","title":"Instructions","text":""},{"location":"Setting_up_a_cluster/#installation-of-a-hello-world-project","title":"Installation of a \"hello-world\" project","text":"<p>For testing, we'll use this \"hello-world\" app: hello-world app</p> <ol> <li>Create a file called <code>hetzner-k3s_cluster_config.yaml</code> with the following configuration. This setup is for a Highly Available (HA) cluster with 3 master nodes and 3 worker nodes. You can use 1 master and 1 worker for testing:</li> </ol> <pre><code>hetzner_token: ...\ncluster_name: hello-world\nkubeconfig_path: \"./kubeconfig\"  # or /cluster/kubeconfig if you are going to use Docker\nk3s_version: v1.32.0+k3s1\n\nnetworking:\n  ssh:\n    port: 22\n    use_agent: false\n    public_key_path: \"~/.ssh/id_rsa.pub\"\n    private_key_path: \"~/.ssh/id_rsa\"\n  allowed_networks:\n    ssh:\n      - 0.0.0.0/0\n    api:\n      - 0.0.0.0/0\n\nmasters_pool:\n  instance_type: cpx22\n  instance_count: 3\n  locations:\n    - fsn1\n    - hel1\n    - nbg1\n\nworker_node_pools:\n- name: small\n  instance_type: cpx22\n  instance_count: 4\n  location: hel1\n- name: big\n  instance_type: cpx32\n  location: fsn1\n  autoscaling:\n    enabled: true\n    min_instances: 0\n    max_instances: 3\n</code></pre> <p>For more details on all the available settings, refer to the full config example in Creating a cluster.</p> <ol> <li>Create the cluster: <code>hetzner-k3s create --config hetzner-k3s_cluster_config.yaml</code></li> <li><code>hetzner-k3s</code> automatically generates a <code>kubeconfig</code> file for the cluster in the directory where you run the tool. You can either copy this file to <code>~/.kube/config</code> if it's the only cluster or run <code>export KUBECONFIG=./kubeconfig</code> in the same directory to access the cluster. After this, you can interact with your cluster using <code>kubectl</code>.</li> </ol> <p>TIP: If you don't want to run <code>kubectl apply ...</code> every time, you can store all your configuration files in a folder and then run <code>kubectl apply -f /path/to/configs/ -R</code>.</p> <ol> <li>Create a file: <code>touch ingress-nginx-annotations.yaml</code></li> <li>Add annotations to the file: <code>nano ingress-nginx-annotations.yaml</code></li> </ol> <pre><code># INSTALLATION\n# 1. Install Helm: https://helm.sh/docs/intro/install/\n# 2. Add ingress-nginx Helm repo: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\n# 3. Update information of available charts: helm repo update\n# 4. Install ingress-nginx:\n# helm upgrade --install \\\n# ingress-nginx ingress-nginx/ingress-nginx \\\n# --set controller.ingressClassResource.default=true \\ # Remove this line if you don\u2019t want Nginx to be the default Ingress Controller\n# -f ./ingress-nginx-annotations.yaml \\\n# --namespace ingress-nginx \\\n# --create-namespace\n\n# LIST of all ANNOTATIONS: https://github.com/hetznercloud/hcloud-cloud-controller-manager/blob/master/internal/annotation/load_balancer.go\n\ncontroller:\n  kind: DaemonSet\n  service:\n    annotations:\n      # Germany:\n      # - nbg1 (Nuremberg)\n      # - fsn1 (Falkenstein)\n      # Finland:\n      # - hel1 (Helsinki)\n      # USA:\n      # - ash (Ashburn, Virginia)\n      # Without this, the load balancer won\u2019t be provisioned and will stay in \"pending\" state.\n      # You can check this state using \"kubectl get svc -n ingress-nginx\"\n      load-balancer.hetzner.cloud/location: nbg1\n\n      # Name of the load balancer. This name will appear in your Hetzner cloud console under \"Your project -&gt; Load Balancers\".\n      # NOTE: This is NOT the load balancer created automatically for HA clusters. You need to specify a different name here to create a separate load balancer for ingress Nginx.\n      load-balancer.hetzner.cloud/name: WORKERS_LOAD_BALANCER_NAME\n\n      # Ensures communication between the load balancer and cluster nodes happens through the private network.\n      load-balancer.hetzner.cloud/use-private-ip: \"true\"\n\n      # [ START: Use these annotations if you care about seeing the actual client IP ]\n      # \"uses-proxyprotocol\" enables the proxy protocol on the load balancer so that the ingress controller and applications can see the real client IP.\n      # \"hostname\" is needed if you use cert-manager (LetsEncrypt SSL certificates). It fixes HTTP01 challenges for cert-manager (https://cert-manager.io/docs/).\n      # Check this link for more details: https://github.com/compumike/hairpin-proxy\n      # In short: the easiest fix provided by some providers (including Hetzner) is to configure the load balancer to use a hostname instead of an IP.\n      load-balancer.hetzner.cloud/uses-proxyprotocol: 'true'\n\n      # 1. \"yourDomain.com\" must be correctly configured in DNS to point to the Nginx load balancer; otherwise, certificate provisioning won\u2019t work.\n      # 2. If you use multiple domains, specify any one.\n      load-balancer.hetzner.cloud/hostname: yourDomain.com\n      # [ END: Use these annotations if you care about seeing the actual client IP ]\n\n      load-balancer.hetzner.cloud/http-redirect-https: 'false'\n</code></pre> <ul> <li>Replace <code>yourDomain.com</code> with your actual domain.</li> <li> <p>Replace <code>WORKERS_LOAD_BALANCER_NAME</code> with a name of your choice.</p> </li> <li> <p>Add the ingress-nginx Helm repo: <code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx</code></p> </li> <li>Update the Helm repo: <code>helm repo update</code></li> <li>Install ingress-nginx:</li> </ul> <pre><code>helm upgrade --install \\\ningress-nginx ingress-nginx/ingress-nginx \\\n--set controller.ingressClassResource.default=true \\\n-f ./ingress-nginx-annotations.yaml \\\n--namespace ingress-nginx \\\n--create-namespace\n</code></pre> <p>The <code>--set controller.ingressClassResource.default=true</code> flag configures this as the default Ingress Class for your cluster. Without this, you\u2019ll need to specify an Ingress Class for every Ingress object you deploy, which can be tedious. If no default is set and you don\u2019t specify one, Nginx will return a 404 Not Found page because it won\u2019t \"pick up\" the Ingress.</p> <p>TIP: To delete it: <code>helm uninstall ingress-nginx -n ingress-nginx</code>. Be careful, as this will delete the current Hetzner load balancer, and installing a new ingress controller may create a new load balancer with a different public IP.</p> <ol> <li> <p>After a few minutes, check that the \"EXTERNAL-IP\" column shows an IP instead of \"pending\": <code>kubectl get svc -n ingress-nginx</code></p> </li> <li> <p>The <code>load-balancer.hetzner.cloud/uses-proxyprotocol: \"true\"</code> annotation requires <code>use-proxy-protocol: \"true\"</code> for ingress-nginx. To set this up, create a file: <code>touch ingress-nginx-configmap.yaml</code></p> </li> <li>Add the following content to the file: <code>nano ingress-nginx-configmap.yaml</code></li> </ol> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  # Do not change the name - this is required by the Nginx Ingress Controller\n  name: ingress-nginx-controller\n  namespace: ingress-nginx\ndata:\n  use-proxy-protocol: \"true\"\n</code></pre> <ol> <li>Apply the ConfigMap: <code>kubectl apply -f ./ingress-nginx-configmap.yaml</code></li> <li>Open your Hetzner cloud console, go to \"Your project -&gt; Load Balancers,\" and find the PUBLIC IP of the load balancer with the name you used in the <code>load-balancer.hetzner.cloud/name: WORKERS_LOAD_BALANCER_NAME</code> annotation. Copy or note this IP.</li> <li>Download the hello-world app: <code>curl https://raw.githubusercontent.com/vitobotta/hetzner-k3s/refs/heads/main/sample-deployment.yaml --output hello-world.yaml</code></li> <li>Edit the file to add the annotation and set the hostname:</li> </ol> <pre><code>---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: hello-world\n  annotations:                       # &lt;&lt;&lt;--- Add annotation\n    kubernetes.io/ingress.class: nginx  # &lt;&lt;&lt;--- Add annotation\nspec:\n  rules:\n  - host: hello-world.IP_FROM_STEP_13.nip.io # &lt;&lt;&lt;--- Replace `IP_FROM_STEP_13` with the IP from step 13.\n  ....\n</code></pre> <ol> <li>Install the hello-world app: <code>kubectl apply -f hello-world.yaml</code></li> <li> <p>Open http://hello-world.IP_FROM_STEP_13.nip.io in your browser. You should see the Rancher \"Hello World!\" page. The <code>host.IP_FROM_STEP_13.nip.io</code> (the <code>.nip.io</code> part is key) is a quick way to test things without configuring DNS. A query to a hostname ending in <code>.nip.io</code> returns the IP address in the hostname itself. If you enabled the proxy protocol as shown earlier, your public IP address should appear in the <code>X-Forwarded-For</code> header, meaning the application can \"see\" it.</p> </li> <li> <p>To connect your actual domain, follow these steps:</p> </li> <li>Assign the IP address from step 13 to your domain in your DNS settings.</li> <li>Change <code>- host: hello-world.IP_FROM_STEP_13.nip.io</code> to <code>- host: yourDomain.com</code>.</li> <li>Run <code>kubectl apply -f hello-world.yaml</code>.</li> <li>Wait until DNS records are updated.</li> </ol>"},{"location":"Setting_up_a_cluster/#if-you-need-letsencrypt","title":"If you need LetsEncrypt","text":"<ol> <li>Add the LetsEncrypt Helm repo: <code>helm repo add jetstack https://charts.jetstack.io</code></li> <li>Update the Helm repo: <code>helm repo update</code></li> <li>Install the LetsEncrypt certificates issuer:</li> </ol> <pre><code>helm upgrade --install \\\n--namespace cert-manager \\\n--create-namespace \\\n--set crds.enabled=true \\\ncert-manager jetstack/cert-manager\n</code></pre> <ol> <li>Create a file called <code>lets-encrypt.yaml</code> with the following content:</li> </ol> <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\n  namespace: cert-manager\nspec:\n  acme:\n    email: [REDACTED]\n    server: https://acme-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      name: letsencrypt-prod-account-key\n    solvers:\n    - http01:\n        ingress:\n          class: nginx\n</code></pre> <ol> <li>Apply the file: <code>kubectl apply -f ./lets-encrypt.yaml</code></li> <li>Edit <code>hello-world.yaml</code> and add the settings for TLS encryption:</li> </ol> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: hello-world\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"  # &lt;&lt;&lt;--- Add annotation\n    kubernetes.io/tls-acme: \"true\"                      # &lt;&lt;&lt;--- Add annotation\nspec:\n  rules:\n  - host: yourDomain.com  # &lt;&lt;&lt;---- Your actual domain\n  tls: # &lt;&lt;&lt;---- Add this block\n  - hosts:\n    - yourDomain.com\n    secretName: yourDomain.com-tls # &lt;&lt;&lt;--- Add reference to secret\n  ....\n</code></pre> <p>TIP: If you didn\u2019t configure Nginx as the default Ingress Class, you\u2019ll need to add the <code>spec.ingressClassName: nginx</code> annotation.</p> <ol> <li>Apply the changes: <code>kubectl apply -f ./hello-world.yaml</code></li> </ol>"},{"location":"Setting_up_a_cluster/#faqs","title":"FAQs","text":""},{"location":"Setting_up_a_cluster/#1-can-i-use-metallb-instead-of-hetzners-load-balancer","title":"1. Can I use MetalLB instead of Hetzner's Load Balancer?","text":"<p>Yes, you can use MetalLB with floating IPs in Hetzner Cloud, but I wouldn\u2019t recommend it. The setup with Hetzner's standard load balancers is much simpler. Plus, load balancers aren\u2019t significantly more expensive than floating IPs, so in my opinion, there\u2019s no real benefit to using MetalLB in this case.</p>"},{"location":"Setting_up_a_cluster/#2-how-do-i-create-and-push-docker-images-to-a-repository-and-how-can-kubernetes-work-with-these-images-gitlab-example","title":"2. How do I create and push Docker images to a repository, and how can Kubernetes work with these images? (GitLab example)","text":"<p>On the machine where you create the image:</p> <ul> <li>Start by logging in to the Docker registry: <code>docker login registry.gitlab.com</code>.</li> <li>Build the Docker image: <code>docker build -t registry.gitlab.com/COMPANY_NAME/REPO_NAME:IMAGE_NAME -f /some/path/to/Dockerfile .</code>.</li> <li>Push the image to the registry: <code>docker push registry.gitlab.com/COMPANY_NAME/REPO_NAME:IMAGE_NAME</code>.</li> </ul> <p>On the machine running Kubernetes:</p> <ul> <li>Generate a secret to allow Kubernetes to access the images: <code>kubectl create secret docker-registry gitlabcreds --docker-server=https://registry.gitlab.com --docker-username=MYUSER --docker-password=MYPWD --docker-email=MYEMAIL -n NAMESPACE_OF_YOUR_APP -o yaml &gt; docker-secret.yaml</code>.</li> <li>Apply the secret: <code>kubectl apply -f docker-secret.yaml -n NAMESPACE_OF_YOUR_APP</code>.</li> </ul>"},{"location":"Setting_up_a_cluster/#3-how-can-i-check-the-resource-usage-of-nodes-or-pods","title":"3. How can I check the resource usage of nodes or pods?","text":"<p>You need the metrics-server installed in your cluster. There are two ways to enable it:</p> <ol> <li> <p>Via hetzner-k3s config (recommended): Enable it in the <code>addons</code> section of your cluster configuration file, and hetzner-k3s will automatically enable it in k3s:    <pre><code>addons:\n  metrics_server:\n    enabled: true\n</code></pre></p> </li> <li> <p>Manual installation: Install via manifest or Helm from the metrics-server repository.</p> </li> </ol> <p>After installation, you can use either <code>kubectl top nodes</code> or <code>kubectl top pods -A</code> to view resource usage.</p>"},{"location":"Setting_up_a_cluster/#4-what-is-ingress","title":"4. What is Ingress?","text":"<p>There are two types of \"ingress\" to understand: <code>Ingress Controller</code> and <code>Ingress Resources</code>.</p> <p>In the case of Nginx:</p> <ul> <li>The <code>Ingress Controller</code> is Nginx itself (defined as <code>kind: Ingress</code>), while <code>Ingress Resources</code> are services (defined as <code>kind: Service</code>).</li> <li>The <code>Ingress Controller</code> has various annotations (rules). You can use these annotations in <code>kind: Ingress</code> to make them \"global\" or in <code>kind: Service</code> to make them \"local\" (specific to that service).</li> <li>The <code>Ingress Controller</code> consists of a Pod and a Service. The Pod runs the Controller, which continuously monitors the /ingresses endpoint in your cluster\u2019s API server for updates to available <code>Ingress Resources</code>.</li> </ul>"},{"location":"Setting_up_a_cluster/#5-how-can-i-configure-autoscaling-to-automatically-set-up-ip-routes-for-new-nodes-to-use-a-nat-server","title":"5. How can I configure autoscaling to automatically set up IP routes for new nodes to use a NAT server?","text":"<p>First, you\u2019ll need a NAT server, as described in this Hetzner community tutorial.</p> <p>Then, use <code>additional_post_k3s_commands</code> to run commands after k3s installation: <pre><code>additional_packages:\n  - ifupdown\nadditional_post_k3s_commands:\n  - apt update\n  - apt upgrade -y\n  - apt autoremove -y\n  - ip route add default via [REDACTED]  # Replace this with your gateway IP\n</code></pre></p> <p>You can also use <code>additional_pre_k3s_commands</code> to run commands before k3s installation if needed.</p>"},{"location":"Setting_up_a_cluster/#useful-commands","title":"Useful Commands","text":"<pre><code>kubectl get service [serviceName] -A or -n [nameSpace]\nkubectl get ingress [ingressName] -A or -n [nameSpace]\nkubectl get pod [podName] -A or -n [nameSpace]\nkubectl get all -A\nkubectl get events -A\nhelm ls -A\nhelm uninstall [name] -n [nameSpace]\nkubectl -n ingress-nginx get svc\nkubectl describe ingress -A\nkubectl describe svc -n ingress-nginx\nkubectl delete configmap nginx-config -n ingress-nginx\nkubectl rollout restart deployment -n NAMESPACE_OF_YOUR_APP\nkubectl get all -A` does not include \"ingress\", so use `kubectl get ing -A\n</code></pre>"},{"location":"Setting_up_a_cluster/#useful-links","title":"Useful Links","text":"<ul> <li>kubectl Cheat Sheet</li> <li>A visual guide on troubleshooting Kubernetes deployments</li> </ul>"},{"location":"Storage/","title":"Storage","text":"<p>hetzner-k3s provides integrated storage solutions for your Kubernetes workloads. The Hetzner CSI Driver is automatically installed during cluster creation, enabling seamless integration with Hetzner's block storage services. If you prefer not to use the driver, you can disable its installation by setting <code>addons.csi_driver.enabled</code> to <code>false</code> in the cluster configuration file. Keep in mind that the minimum size for a volume is 10 Gi.</p>"},{"location":"Storage/#overview","title":"Overview","text":"<p>Two storage classes are available:</p> <ol> <li><code>hcloud-volumes</code> (default): Uses Hetzner's block storage based on Ceph, providing replicated and highly available storage</li> <li><code>local-path</code>: Uses local node storage for maximum IOPS performance (disabled by default)</li> </ol>"},{"location":"Storage/#hetzner-block-storage-hcloud-volumes","title":"Hetzner Block Storage (hcloud-volumes)","text":""},{"location":"Storage/#features","title":"Features","text":"<ul> <li>Replicated: Based on Ceph, ensuring data is replicated across multiple disks</li> <li>Highly Available: Redundant storage with no single point of failure</li> <li>Minimum Size: 10Gi (smaller requests will be automatically rounded up)</li> <li>Maximum Size: 10Ti per volume</li> <li>Dynamic Provisioning: Volumes are automatically created and attached when needed</li> </ul>"},{"location":"Storage/#basic-usage","title":"Basic Usage","text":"<p>Create a Persistent Volume Claim (PVC) using the default storage class:</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-data-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre> <p>This will automatically provision a 10Gi Hetzner volume and attach it to the pod that uses this PVC.</p>"},{"location":"Storage/#example-wordpress-with-persistent-storage","title":"Example: WordPress with Persistent Storage","text":"<pre><code>---\n# Persistent Volume Claim for WordPress\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: wordpress-pvc\n  labels:\n    app: wordpress\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 20Gi\n---\n# Persistent Volume Claim for MySQL\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mysql-pvc\n  labels:\n    app: mysql\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n---\n# MySQL Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql\n  labels:\n    app: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: mysql\n        image: mysql:8.0\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: \"rootpassword\"\n        - name: MYSQL_DATABASE\n          value: \"wordpress\"\n        - name: MYSQL_USER\n          value: \"wordpress\"\n        - name: MYSQL_PASSWORD\n          value: \"wordpress\"\n        ports:\n        - containerPort: 3306\n        volumeMounts:\n        - name: mysql-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-storage\n        persistentVolumeClaim:\n          claimName: mysql-pvc\n---\n# WordPress Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wordpress\n  labels:\n    app: wordpress\nspec:\n  selector:\n    matchLabels:\n      app: wordpress\n  template:\n    metadata:\n      labels:\n        app: wordpress\n    spec:\n      containers:\n      - name: wordpress\n        image: wordpress:latest\n        env:\n        - name: WORDPRESS_DB_HOST\n          value: \"mysql\"\n        - name: WORDPRESS_DB_USER\n          value: \"wordpress\"\n        - name: WORDPRESS_DB_PASSWORD\n          value: \"wordpress\"\n        - name: WORDPRESS_DB_NAME\n          value: \"wordpress\"\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: wordpress-storage\n          mountPath: /var/www/html\n      volumes:\n      - name: wordpress-storage\n        persistentVolumeClaim:\n          claimName: wordpress-pvc\n</code></pre>"},{"location":"Storage/#local-path-storage","title":"Local Path Storage","text":""},{"location":"Storage/#overview_1","title":"Overview","text":"<p>The Local Path storage class uses the node's local disk storage directly, providing higher IOPS and lower latency compared to network-attached storage. This is ideal for:</p> <ul> <li>High-performance databases (Redis, MongoDB, PostgreSQL)</li> <li>Caching systems</li> <li>Temporary storage</li> <li>Applications requiring maximum storage performance</li> </ul>"},{"location":"Storage/#enabling-local-path-storage","title":"Enabling Local Path Storage","text":"<p>To enable the <code>local-path</code> storage class, add this to your cluster configuration:</p> <pre><code>addons:\n  local_path_storage_class:\n    enabled: true\n</code></pre>"},{"location":"Storage/#usage-example","title":"Usage Example","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: redis-cache-pvc\nspec:\n  storageClassName: local-path\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis:alpine\n        ports:\n        - containerPort: 6379\n        volumeMounts:\n        - name: redis-data\n          mountPath: /data\n      volumes:\n      - name: redis-data\n        persistentVolumeClaim:\n          claimName: redis-cache-pvc\n</code></pre>"},{"location":"Storage/#important-considerations","title":"Important Considerations","text":"<p>Advantages of Local Path Storage: - Higher Performance: No network overhead, direct disk access - Lower Latency: Faster read/write operations - Reduced Cost: No additional costs for network storage</p> <p>Limitations of Local Path Storage: - Not Highly Available: Data is tied to specific nodes - No Replication: Data loss occurs if node fails, so it works best when the application takes care of replication already - Limited to Single Node: Pod can only be scheduled on the node where data resides - Manual Migration: Data must be manually migrated when moving pods</p>"},{"location":"Storage/#storage-class-comparison","title":"Storage Class Comparison","text":"Feature hcloud-volumes local-path High Availability \u2705 Yes \u274c No Data Replication \u2705 Yes \u274c No Performance Good (Network) Excellent (Local) Maximum Size 10Ti Limited by node disk Cost Volume pricing Included in instance Use Case Persistent data Caching, temporary data, high-performance apps Pod Migration \u2705 Easy \u274c Manual"},{"location":"Storage/#advanced-storage-features","title":"Advanced Storage Features","text":""},{"location":"Storage/#volume-expansion","title":"Volume Expansion","text":"<p>You can expand existing volumes online without downtime:</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-expandable-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre> <p>To expand the volume:</p> <pre><code># Edit the PVC to increase the size\nkubectl edit pvc my-expandable-pvc\n\n# Change storage: 10Gi to storage: 20Gi\n</code></pre> <p>The CSI driver will automatically resize the filesystem if supported.</p>"},{"location":"Storage/#storage-best-practices","title":"Storage Best Practices","text":""},{"location":"Storage/#1-choose-the-right-storage-type","title":"1. Choose the Right Storage Type","text":"<ul> <li>Use <code>hcloud-volumes</code> for:</li> <li>Production databases</li> <li>Persistent application data</li> <li>Content that must survive pod restarts</li> <li> <p>Applications requiring high availability</p> </li> <li> <p>Use <code>local-path</code> for:</p> </li> <li>Caching layers (Redis, Memcached) and databases (Postgres, MySQL)</li> <li>Temporary file storage</li> <li>High-performance computing workloads</li> <li>Applications that can tolerate data loss</li> </ul>"},{"location":"Storage/#2-monitor-storage-usage","title":"2. Monitor Storage Usage","text":"<pre><code># Check PVC usage\nkubectl get pvc -A\nkubectl describe pvc &lt;pvc-name&gt;\n\n# Check actual disk usage on nodes\nkubectl get nodes -o wide\nssh root@&lt;node-ip&gt; 'df -h'\n</code></pre>"},{"location":"Storage/#3-set-resource-limits","title":"3. Set Resource Limits","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: monitored-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n    # Optional: Set resource limits (enforced by storage class)\n    limits:\n      storage: 20Gi\n</code></pre>"},{"location":"Storage/#4-use-storage-classes-appropriately","title":"4. Use Storage Classes Appropriately","text":"<pre><code># Explicitly specify storage class\nspec:\n  storageClassName: hcloud-volumes  # or local-path\n</code></pre>"},{"location":"Storage/#5-implement-backup-strategies","title":"5. Implement Backup Strategies","text":"<ul> <li>Application-level Backups: Implement regular backups using tools like velero, restic, or application-specific backup solutions</li> <li>Off-server Backups: Ensure critical data is backed up to external storage or cloud storage</li> <li>Monitoring: Set up alerts for storage usage and disk space</li> </ul>"},{"location":"Storage/#troubleshooting-storage-issues","title":"Troubleshooting Storage Issues","text":""},{"location":"Storage/#pvc-stuck-in-pending","title":"PVC Stuck in Pending","text":"<ol> <li> <p>Check Storage Class:    <pre><code>kubectl get sc\n</code></pre></p> </li> <li> <p>Verify PVC Definition:    <pre><code>kubectl describe pvc &lt;pvc-name&gt;\n</code></pre></p> </li> <li> <p>Check CSI Driver Status:    <pre><code>kubectl get pods -n kube-system | grep csi\nkubectl logs -n kube-system &lt;csi-pod-name&gt;\n</code></pre></p> </li> </ol>"},{"location":"Storage/#volume-mount-failures","title":"Volume Mount Failures","text":"<ol> <li> <p>Check Volume Attachment:    <pre><code>kubectl get pv\nkubectl describe pv &lt;pv-name&gt;\n</code></pre></p> </li> <li> <p>Verify Pod Definition:    <pre><code>kubectl describe pod &lt;pod-name&gt;\n</code></pre></p> </li> <li> <p>Check Node Capacity:    <pre><code>kubectl describe node &lt;node-name&gt;\n</code></pre></p> </li> </ol>"},{"location":"Storage/#performance-issues","title":"Performance Issues","text":"<ol> <li> <p>Monitor I/O:    <pre><code>kubectl exec -it &lt;pod-name&gt; -- iostat -x 1\n</code></pre></p> </li> <li> <p>Check Storage Type: Ensure you're using the right storage class for your workload</p> </li> <li> <p>Consider Local Storage: For high-performance workloads, consider switching to <code>local-path</code></p> </li> </ol>"},{"location":"Storage/#cost-optimization","title":"Cost Optimization","text":""},{"location":"Storage/#hcloud-volumes-costs","title":"hcloud-volumes Costs","text":"<ul> <li>Monthly Charge: Based on volume size (\u20ac0.04/GB per month)</li> </ul>"},{"location":"Storage/#optimization-strategies","title":"Optimization Strategies","text":"<ol> <li>Right-size Volumes: Start with smaller sizes and expand as needed</li> <li>Use Local Storage: For temporary or high-performance data</li> <li>Monitor Usage: Identify and reclaim unused storage</li> </ol>"},{"location":"Storage/#monitoring-commands","title":"Monitoring Commands","text":"<pre><code># Check storage usage across all namespaces\nkubectl get pvc -A --no-headers | awk '{print $4, $5}'\n\n# List all storage classes\nkubectl get sc\n\n# Check CSI driver pods\nkubectl get pods -n kube-system | grep -E '(csi|storage)'\n\n# Check volume health\nkubectl get pv -o custom-columns=NAME:.metadata.name,STATUS:.status.phase,CAPACITY:.spec.capacity.storage,STORAGE-CLASS:.spec.storageClassName\n</code></pre>"},{"location":"Troubleshooting/","title":"Troubleshooting","text":"<p>This page covers common issues and their solutions. If you don't find an answer here, check GitHub Issues or ask in GitHub Discussions.</p>"},{"location":"Troubleshooting/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"Troubleshooting/#ssh-connection-problems","title":"SSH Connection Problems","text":"<p>If the tool stops working after creating instances and you experience timeouts, the issue might be related to your SSH key. This can happen if you're using a key with a passphrase or an older key, as newer operating systems may no longer support certain encryption methods.</p> <p>Solutions: 1. Enable SSH Agent: Set <code>networking.ssh.use_agent</code> to <code>true</code> in your configuration file. This lets the SSH agent manage the key.</p> <p>For macOS:    <pre><code>eval \"$(ssh-agent -s)\"\nssh-add --apple-use-keychain ~/.ssh/&lt;private key&gt;\n</code></pre></p> <p>For Linux:    <pre><code>eval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/&lt;private key&gt;\n</code></pre></p> <ol> <li> <p>Test SSH Manually: Verify you can SSH to the instances manually:    <pre><code>ssh -i ~/.ssh/your_private_key root@&lt;server_ip&gt;\n</code></pre></p> </li> <li> <p>Check Key Permissions: Ensure your private key has correct permissions:    <pre><code>chmod 600 ~/.ssh/your_private_key\n</code></pre></p> </li> </ol>"},{"location":"Troubleshooting/#enable-debug-mode","title":"Enable Debug Mode","text":"<p>You can run <code>hetzner-k3s</code> with the <code>DEBUG</code> environment variable set to <code>true</code> for more detailed output:</p> <pre><code>DEBUG=true hetzner-k3s create --config cluster_config.yaml\n</code></pre> <p>This will provide more detailed output, which can help you identify the root of the problem.</p>"},{"location":"Troubleshooting/#cluster-creation-fails-after-node-creation","title":"Cluster Creation Fails after Node Creation","text":"<p>Symptoms: Instances are created but cluster setup fails.</p> <p>Possible Causes: - Network connectivity issues between nodes - Firewall blocking communication - Hetzner API rate limits</p> <p>Solutions: 1. Check Network Connectivity: Verify nodes can communicate with each other 2. Review Firewall Rules: Ensure necessary ports are open 3. Wait and Retry: If it's a rate limit issue, wait a few minutes and retry 4. Check Network Configuration: See section below for IPv4/IPv6 configuration issues</p>"},{"location":"Troubleshooting/#ipv4-disabled-with-ipv6-only-configuration","title":"IPv4 Disabled with IPv6 Only Configuration","text":"<p>Symptoms: Cluster creation hangs after nodes are created. SSH connection times out when trying to connect to a private IP address.</p> <p>Note: The tool currently does not support IPv6-only public network configuration. When you disable IPv4 (<code>public_network.ipv4: false</code>), you must run <code>hetzner-k3s</code> from a machine that has access to the same private network, either directly or through a VPN. Otherwise, the tool will attempt to use the private IP addresses for SSH connections and fail.</p>"},{"location":"Troubleshooting/#load-balancer-issues","title":"Load Balancer Issues","text":"<p>Symptoms: Load balancer stuck in \"pending\" state</p> <p>Solutions: 1. Check Annotations: Ensure proper annotations are set on your services 2. Verify Location: Make sure the load balancer location matches your node locations 3. Check DNS Configuration: If using hostname annotation, ensure DNS is properly configured</p>"},{"location":"Troubleshooting/#node-not-ready","title":"Node Not Ready","text":"<p>Symptoms: Nodes show up as <code>NotReady</code> status</p> <p>Solutions: 1. Check Node Status:    <pre><code>kubectl describe node &lt;node-name&gt;\nkubectl get nodes -o wide\n</code></pre></p> <ol> <li> <p>Check Kubelet:    <pre><code>ssh -i ~/.ssh/your_private_key root@&lt;node-ip&gt;\nsystemctl status k3s-agent  # for workers\nsystemctl status k3s-server  # for masters\njournalctl -u k3s-agent -f\n</code></pre></p> </li> <li> <p>Restart K3s:    <pre><code>ssh -i ~/.ssh/your_private_key root@&lt;node-ip&gt;\nsystemctl restart k3s-agent  # or k3s-server\n</code></pre></p> </li> </ol>"},{"location":"Troubleshooting/#pod-stuck-in-pending-state","title":"Pod Stuck in Pending State","text":"<p>Symptoms: Pods remain in <code>Pending</code> state indefinitely</p> <p>Solutions: 1. Check Resource Availability:    <pre><code>kubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;\n</code></pre>    Look for events indicating insufficient resources.</p> <ol> <li> <p>Add More Nodes: If nodes are at capacity, either scale up existing node pools or add new nodes</p> </li> <li> <p>Check Taints and Tolerations: Ensure pods have tolerations for any node taints</p> </li> </ol>"},{"location":"Troubleshooting/#storage-issues","title":"Storage Issues","text":"<p>Symptoms: PVCs stuck in <code>Pending</code> state, pods can't mount volumes</p> <p>Solutions: 1. Check Storage Classes:    <pre><code>kubectl get sc\n</code></pre></p> <ol> <li> <p>Describe PVC:    <pre><code>kubectl describe pvc &lt;pvc-name&gt; -n &lt;namespace&gt;\n</code></pre></p> </li> <li> <p>Check CSI Driver:    <pre><code>kubectl get pods -n kube-system | grep csi\n</code></pre></p> </li> </ol>"},{"location":"Troubleshooting/#network-plugin-issues","title":"Network Plugin Issues","text":"<p>Symptoms: Pods can't communicate with each other, DNS resolution fails</p> <p>Solutions: 1. Check CNI Pods:    <pre><code>kubectl get pods -n kube-system | grep -E '(flannel|cilium)'\n</code></pre></p> <ol> <li>Restart CNI: Restart the relevant CNI pods</li> </ol>"},{"location":"Troubleshooting/#upgrade-issues","title":"Upgrade Issues","text":"<p>Symptoms: Cluster upgrade process gets stuck</p> <p>Solutions: 1. Clean up Upgrade Resources:    <pre><code>kubectl -n system-upgrade delete job --all\nkubectl -n system-upgrade delete plan --all\n</code></pre></p> <ol> <li> <p>Remove Labels:    <pre><code>kubectl label node --all plan.upgrade.cattle.io/k3s-server- plan.upgrade.cattle.io/k3s-agent-\n</code></pre></p> </li> <li> <p>Restart Upgrade Controller:    <pre><code>kubectl -n system-upgrade rollout restart deployment system-upgrade-controller\n</code></pre></p> </li> </ol>"},{"location":"Troubleshooting/#getting-help","title":"Getting Help","text":"<p>If you're still experiencing issues after trying these solutions:</p> <ol> <li>Check GitHub Issues: Search existing issues at github.com/vitobotta/hetzner-k3s/issues</li> <li>Create New Issue: If your issue hasn't been reported, create a new issue with:</li> <li>Your configuration file (redacted)</li> <li>Full debug output (<code>DEBUG=true hetzner-k3s ...</code>)</li> <li>Operating system and Hetzner-k3s version</li> <li>Steps to reproduce the issue</li> <li>GitHub Discussions: For general questions and discussions, use GitHub Discussions</li> </ol>"},{"location":"Troubleshooting/#useful-commands-for-troubleshooting","title":"Useful Commands for Troubleshooting","text":"<pre><code># Check cluster status\nkubectl cluster-info\nkubectl get nodes\nkubectl get pods -A\n\n# Check resource usage\nkubectl top nodes\nkubectl top pods -A\n\n# Check events\nkubectl get events -A --sort-by='.metadata.creationTimestamp'\n\n# Check specific pod details\nkubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;\nkubectl logs &lt;pod-name&gt; -n &lt;namespace&gt;\n\n# Check node details\nkubectl describe node &lt;node-name&gt;\n\n# Check network connectivity\nkubectl run test-pod --image=busybox -- sleep 3600\nkubectl exec -it test-pod -- nslookup kubernetes.default\nkubectl exec -it test-pod -- ping &lt;other-pod-ip&gt;\n</code></pre>"},{"location":"Upgrading_a_cluster_from_1x_to_2x/","title":"Upgrading a cluster created with hetzner-k3s v1.x to v2.x","text":"<p>The v1 version of hetzner-k3s is quite old and hasn\u2019t been supported for some time. I understand that many haven\u2019t upgraded to v2 because, until now, there wasn\u2019t a simple process to do this.</p> <p>The good news is that the migration is now possible and straightforward, as long as you follow these instructions very carefully and take your time. This upgrade also allows you to replace deprecated instance types with newer ones. Note that this migration requires hetzner-k3s v2.2.4 or higher. Using the very latest version is recommended.</p>"},{"location":"Upgrading_a_cluster_from_1x_to_2x/#prerequisites","title":"Prerequisites","text":"<ul> <li> I suggest installing the hcloud utility. It will make it easier and faster to delete old master nodes.</li> </ul>"},{"location":"Upgrading_a_cluster_from_1x_to_2x/#upgrading-configuration-and-first-steps","title":"Upgrading configuration and first steps","text":"<ul> <li> Backup apps and data \u2013 As with any migration, there\u2019s some risk involved, so it\u2019s better to be prepared in case things don\u2019t go as planned.</li> <li> Backup kubeconfig and the old config file</li> <li> Uninstall the System Upgrade Controller</li> <li> Create a resolv file on existing nodes. You can do this manually or automate it using the <code>hcloud</code> CLI: <pre><code>hcloud server list | awk '{print $4}' | tail -n +2 | while read ip; do\n  echo \"Setting DNS for ${ip}\"\n  ssh -n root@${ip} \"echo nameserver 8.8.8.8 | tee /etc/k8s-resolv.conf\"\n  ssh -n root@${ip} \"cat /etc/k8s-resolv.conf\"\ndone\n</code></pre></li> <li> Convert the config file to the new format. You can find guidance here for the initial v2.0.0 release. Make sure you read all the following release notes to apply any other changes to the config format introduced by newer versions of the tool, until the very latest release.</li> <li> Remove or comment out empty node pools from the config file.</li> <li> Set <code>embedded_registry_mirror</code>.<code>enabled</code> to <code>false</code> if necessary, depending on the current version of k3s (refer to this documentation).</li> <li> Add <code>legacy_instance_type</code> to ALL node pools, including both masters and workers. Set it to the current instance type for each node pool, even if it\u2019s now deprecated.  This step is critical for the migration.</li> <li> Set <code>instance</code> type for all the node pools to supported instance types, if your current ones have been deprecated by Hetzner. This is important to replace the existing nodes with new ones based on supported instance types.</li> <li> Run the <code>create</code> command using the latest version of hetzner-k3s and the new config file.</li> <li> Wait for all CSI pods in <code>kube-system</code> to restart, and make sure everything is running correctly.</li> </ul>"},{"location":"Upgrading_a_cluster_from_1x_to_2x/#rotating-control-plane-instances-with-the-new-instance-type","title":"Rotating control plane instances with the new instance type","text":"<p>Replace one master at a time (unless your cluster has a load balancer for the Kubernetes API, switch to another master's kube context before replacing <code>master1</code>):</p> <ul> <li> Drain the master first, then delete it both from the cluster using both kubectl and from the Hetzner console (or the <code>hcloud</code> CLI) to delete the actual instance.</li> <li> Rerun the <code>create</code> command to recreate the master with the new instance type. Wait for it to join the control plane and reach the \"ready\" status.</li> <li> SSH into each master and verify that the etcd members are updated and in sync:</li> </ul> <pre><code>sudo apt-get update\nsudo apt-get install etcd-client\n\nexport ETCDCTL_API=3\nexport ETCDCTL_ENDPOINTS=https://127.0.0.1:2379\nexport ETCDCTL_CACERT=/var/lib/rancher/k3s/server/tls/etcd/server-ca.crt\nexport ETCDCTL_CERT=/var/lib/rancher/k3s/server/tls/etcd/server-client.crt\nexport ETCDCTL_KEY=/var/lib/rancher/k3s/server/tls/etcd/server-client.key\n\netcdctl member list\n</code></pre> <p>Repeat the steps above carefully for each master. Once all three masters have been replaced:</p> <ul> <li> Rerun the <code>create</code> command once or twice more to ensure the configuration is stable and the masters no longer restart.</li> <li> Debug DNS resolution. If there are issues, restart the agents for DNS resolution with this command, then restart CoreDNS: <pre><code>hcloud server list | grep worker | awk '{print $4}'| while read ip; do\n  echo \"${ip}\"\n  ssh -n root@${ip} \"systemctl restart k3s-agent\"\n  sleep 10\ndone\n</code></pre></li> <li> Address any issues with your workloads before proceeding to rotate the worker nodes.</li> </ul>"},{"location":"Upgrading_a_cluster_from_1x_to_2x/#rotating-a-worker-node-pool","title":"Rotating a worker node pool","text":"<ul> <li> Increase the node count for the pool by 1.</li> <li> Run the <code>create</code> command to create the extra node needed during the pool rotation.</li> </ul> <p>Replace one worker node at a time (except for the last one you just added):</p> <ul> <li> Drain a node.</li> <li> Delete the drained node using both kubectl and the Hetzner console (or the <code>hcloud</code> CLI).</li> <li> Rerun the <code>create</code> command to recreate the deleted node.</li> <li> Verify everything is working as expected before moving on to the next node in the pool.</li> </ul> <p>Once all the existing nodes in the pool have been rotated:</p> <ul> <li> Drain the very last node in the pool (the one you added earlier).</li> <li> Verify everything is functioning correctly.</li> <li> Delete the last node using both kubectl and the Hetzner console (or the <code>hcloud</code> CLI).</li> <li> Update the <code>instance_count</code> for the node pool by reducing it by 1.</li> <li> Proceed with the next node pool.</li> </ul>"},{"location":"Upgrading_a_cluster_from_1x_to_2x/#finalizing","title":"Finalizing","text":"<ul> <li> Remove the <code>legacy_instance_type</code> setting from both master and worker node pools.</li> <li> Rerun the <code>create</code> command once more to double-check everything.</li> <li> Optionally, convert the currently zonal cluster to a regional one with masters in different locations (see this guide).</li> </ul>"}]}